{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rollout/\n",
    "ep_len_mean: Mean episode length (averaged over stats_window_size episodes, 100 by default)\n",
    "\n",
    "ep_rew_mean: Mean episodic training reward (averaged over stats_window_size episodes, 100 by default), a Monitor wrapper is required to compute that value (automatically added by make_vec_env).\n",
    "\n",
    "exploration_rate: Current value of the exploration rate when using DQN, it corresponds to the fraction of actions taken randomly (epsilon of the “epsilon-greedy” exploration)\n",
    "\n",
    "success_rate: Mean success rate during training (averaged over stats_window_size episodes, 100 by default), you must pass an extra argument to the Monitor wrapper to log that value (info_keywords=(\"is_success\",)) and provide info[\"is_success\"]=True/False on the final step of the episode\n",
    "\n",
    "time/\n",
    "episodes: Total number of episodes\n",
    "\n",
    "fps: Number of frames per seconds (includes time taken by gradient update)\n",
    "\n",
    "iterations: Number of iterations (data collection + policy update for A2C/PPO)\n",
    "\n",
    "time_elapsed: Time in seconds since the beginning of training\n",
    "\n",
    "total_timesteps: Total number of timesteps (steps in the environments)\n",
    "\n",
    "train/\n",
    "actor_loss: Current value for the actor loss for off-policy algorithms\n",
    "\n",
    "approx_kl: approximate mean KL divergence between old and new policy (for PPO), it is an estimation of how much changes happened in the update\n",
    "\n",
    "clip_fraction: mean fraction of surrogate loss that was clipped (above clip_range threshold) for PPO.\n",
    "\n",
    "clip_range: Current value of the clipping factor for the surrogate loss of PPO\n",
    "\n",
    "critic_loss: Current value for the critic function loss for off-policy algorithms, usually error between value function output and TD(0), temporal difference estimate\n",
    "\n",
    "ent_coef: Current value of the entropy coefficient (when using SAC)\n",
    "\n",
    "ent_coef_loss: Current value of the entropy coefficient loss (when using SAC)\n",
    "\n",
    "entropy_loss: Mean value of the entropy loss (negative of the average policy entropy)\n",
    "\n",
    "explained_variance: Fraction of the return variance explained by the value function, see https://scikit-learn.org/stable/modules/model_evaluation.html#explained-variance-score (ev=0 => might as well have predicted zero, ev=1 => perfect prediction, ev<0 => worse than just predicting zero)\n",
    "\n",
    "learning_rate: Current learning rate value\n",
    "\n",
    "loss: Current total loss value\n",
    "\n",
    "n_updates: Number of gradient updates applied so far\n",
    "\n",
    "policy_gradient_loss: Current value of the policy gradient loss (its value does not have much meaning)\n",
    "\n",
    "value_loss: Current value for the value function loss for on-policy algorithms, usually error between value function output and Monte-Carlo estimate (or TD(lambda) estimate)\n",
    "\n",
    "std: Current standard deviation of the noise when using generalized State-Dependent Exploration (gSDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to model\\Baselines_default_MountainCar_20240104_092652\\data\\log\\PPO_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -200     |\n",
      "| time/              |          |\n",
      "|    fps             | 423      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -200         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 300          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030605602 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.000438     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.3         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000884    |\n",
      "|    value_loss           | 137          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -200          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 280           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 21            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037818044 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -0.0506       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 13.1          |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | 1.71e-05      |\n",
      "|    value_loss           | 90.2          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -200          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 277           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 29            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024033195 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.1          |\n",
      "|    explained_variance   | -0.0253       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 16.4          |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | 4.54e-05      |\n",
      "|    value_loss           | 85.5          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -200         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 277          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021127455 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.00348     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.6         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000538    |\n",
      "|    value_loss           | 70.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -200        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 265         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007811274 |\n",
      "|    clip_fraction        | 0.0022      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 5.61e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.56        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    value_loss           | 55.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -200        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 264         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011126519 |\n",
      "|    clip_fraction        | 0.0127      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.00312     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.46        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    value_loss           | 42.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -200        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008905963 |\n",
      "|    clip_fraction        | 0.00229     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.000272   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.23        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00135    |\n",
      "|    value_loss           | 31.3        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 200           |\n",
      "|    ep_rew_mean          | -200          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 256           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 71            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086811476 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.09         |\n",
      "|    explained_variance   | -0.00282      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.2           |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | 4.58e-05      |\n",
      "|    value_loss           | 21.9          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -200        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 252         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 81          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006118318 |\n",
      "|    clip_fraction        | 0.00347     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -0.00101    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 14.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -200         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025245072 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | -0.00235     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.439        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00082     |\n",
      "|    value_loss           | 9.37         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -200         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034272464 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.00126     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.65         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000941    |\n",
      "|    value_loss           | 5.92         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | -200        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012749096 |\n",
      "|    clip_fraction        | 0.0217      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -0.000598   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.255       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00273    |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 200          |\n",
      "|    ep_rew_mean          | -200         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0145247355 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.000302     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.195        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00459     |\n",
      "|    value_loss           | 2.4          |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Parallel environments\n",
    "# vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "#Single vectorized enviroment\n",
    "env = make_vec_env(\"MountainCar-v0\")\n",
    "\n",
    "\n",
    "#create model and logging the callbacks\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'model\\\\Baselines_default_MountainCar_{datum_uhrzeit}\\\\data'\n",
    "\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "log_dir = f\"{savedir}\\\\log\"\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log= log_dir)    #verbose 0-> no output, verbose 1-> info messages, verbose 2-> debug messages\n",
    "\n",
    "        \n",
    "model.learn(total_timesteps=1000000) \n",
    "model.save(savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error: Unexpected observation shape (1, 2) for Box environment, please use (4,) or (n_env, 4) for the observation shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[0;32m      8\u001b[0m     obs, rewards, dones, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      9\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:553\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    535\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    539\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:363\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m     )\n\u001b[1;32m--> 363\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    366\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(obs_tensor, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:270\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    266\u001b[0m     observation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(observation)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;66;03m# Dict obs need to be handled separately\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m     vectorized_env \u001b[38;5;241m=\u001b[39m is_vectorized_observation(observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     observation \u001b[38;5;241m=\u001b[39m observation\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\utils.py:399\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[1;34m(observation, observation_space)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[38;5;129;01min\u001b[39;00m is_vec_obs_func_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, space_type):\n\u001b[1;32m--> 399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m is_vec_obs_func(observation, observation_space)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# for-else happens if no break is called\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\utils.py:266\u001b[0m, in \u001b[0;36mis_vectorized_box_observation\u001b[1;34m(observation, observation_space)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Unexpected observation shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox environment, please use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor (n_env, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) for the observation shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, observation_space\u001b[38;5;241m.\u001b[39mshape)))\n\u001b[0;32m    270\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Error: Unexpected observation shape (1, 2) for Box environment, please use (4,) or (n_env, 4) for the observation shape."
     ]
    }
   ],
   "source": [
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(savedir)\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<stable_baselines3.common.buffers.RolloutBuffer object at 0x000001976D2EBCD0>\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"ppo_mountain_cart_v0\")\n",
    "\n",
    "print(model.rollout_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
