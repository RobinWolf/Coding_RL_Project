{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b358710c-7fbd-482a-84a8-d220cda6de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "from keras.callbacks import TensorBoard\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import pygame\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041b7af-9a74-45db-b58b-d4433987d60f",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "PPO is a policy gradient Actor-Critic algorithm. The policy model, the **actor** network  produces a stochastic policy. It maps the state to a probability distribution over the set of possible actions. The **critic** network is used to approximate the value function and then, the advantage is calculated:\n",
    "\n",
    "$$\n",
    "A_\\Phi (s_t, a_t) = q_\\Phi (s_t,a_t) - v_\\Phi (s_t) = R_t + \\gamma v_{\\Phi'} (s_{t+1}) - v_\\Phi (s_t)\n",
    "$$\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b5d89-2006-432e-a7fd-9c6eb930c5b0",
   "metadata": {},
   "source": [
    "PPO uses 2 main models. The actor network learns the stochastic policy. It maps the state to a probability distribution over the set of possible actions. The critic network learns the value function. It maps the state to a scalar.\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9179d247-1c6d-4307-bea3-def396311e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change inheritance class to enable saving the models\n",
    "\n",
    "# *************************************************MODIFIED************************************************** \n",
    "class Actor(tf.keras.Model):    # represents/ approximates the stochastic-policy (policy = weights from the nn)\n",
    "    def __init__(self, units=(400, 300), n_actions=1, **kwargs):    # input = observation shape(batchsize, observation_shape) -> same as in discrete action space \n",
    "        super(Actor, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        n_outputs = n_actions*2 # one continuous output distribution contains values std and mean for gaussian \n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(n_outputs))   # output = ?? shape(batchsize, n_outputs)\n",
    "        # modify output dimension to n_actions * 2 (= 1 for MountainCarCont) -> output is now std and mean of continuous gaussian distribution\n",
    "        # modify output layer activation function -> use no activation/ linear activation a(x) = x to output the estimated values directly\n",
    "        # if custom clipping is necessary, use tanh as output activation function to clip[-1,1]\n",
    "        # in discrete action space 'softmax' exp(x) / tf.reduce_sum(exp(x)) calculates the value of each output vector in that way, the output can be interpreted as a discrete probability distribution (sum vectors = 1)\n",
    "        \n",
    "    # forward pass through the network\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        # if last layer is reached, prepare the output to return mean and std\n",
    "        mean, log_std = tf.split(outputs, 2, axis=-1)  # Split the output(Tensor shape(batchsize, n_outputs)) into 2 tensors (mean and log_std (ln!)) along the last axis(collums)\n",
    "        return mean, log_std\n",
    "\n",
    "    \n",
    "class Critic(tf.keras.Model):   # evaluates choosen actions(critic) in reference to the estimated actions(target critic) -> provides feedback to the actor (optipizing the policy was better/ worser)\n",
    "    def __init__(self, units=(400, 300), **kwargs):\n",
    "        super(Critic, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(1))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class PPOAgent:\n",
    "    def __init__(self, action_space, observation_space, gamma=0.99, epsilon = 0.1):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.observation_shape = observation_space.shape[0]\n",
    "# *************************************************MODIFIED**************************************************     \n",
    "        self.action_shape = action_space.shape[0]\n",
    "        self.actor = Actor(n_actions=self.action_shape)\n",
    "        self.actor_old = Actor(n_actions=self.action_shape)\n",
    "# ***********************************************************************************************************     \n",
    "\n",
    "        self.critic = Critic()\n",
    "        self.target_critic = Critic()\n",
    "        \n",
    "        self.actor_learning_rate=0.00025\n",
    "        self.critic_learning_rate=0.001\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(self.actor_learning_rate)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.critic_learning_rate)   # default = 0,001 -> hatten wir auch schon\n",
    "        self._init_networks()\n",
    "        \n",
    "    def _init_networks(self):\n",
    "        initializer = np.zeros([1, self.observation_shape])   # ergänzt zu V1 -> hatten wir aber auch schon gemacht\n",
    "        self.actor(initializer)\n",
    "        self.actor_old(initializer)\n",
    "        \n",
    "        self.critic(initializer)\n",
    "        self.target_critic(initializer)\n",
    "        \n",
    "        self.update_frozen_nets()\n",
    "\n",
    " # *************************************************MODIFIED**************************************************     \n",
    "    def act(self, observation):\n",
    "        mean_tensor, log_std_tensor = self.actor(observation) # Actor network will output the gaussian probability distribution of actions for the given observation-state\n",
    "        mean = tf.gather(mean_tensor, indices= 0, axis=1).numpy()\n",
    "        std = tf.gather(tf.exp(log_std_tensor), indices= 0, axis=1).numpy()\n",
    "        #print('mean', mean, 'std', std)\n",
    "        action = np.random.normal(size = self.action_shape, loc = mean, scale = std)  # modify sampling method to sample random actions in respect a continous normal distribution\n",
    "        #print('Choosen Action:', action)\n",
    "        return action\n",
    "        # in continous action space this output should be a real number (optional clipped [-1,1])\n",
    "        # for mnt car = force applied on the car (clipped [-1,1]) and * power od 0.0015 -> no custom action clipping necessary\n",
    " # ***********************************************************************************************************     \n",
    "   \n",
    "    def get_critic_grads(self, states, rewards, next_states, dones):    # parameters are adjusted to minimize the difference between predicted values and observed returns\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_value = self.target_critic(next_states)\n",
    "            q_value = rewards + (1-dones) * self.gamma * next_value\n",
    "            value = self.critic(states)\n",
    "            \n",
    "            advantage = q_value - value\n",
    "            loss = tf.reduce_mean(tf.square(advantage))\n",
    "        gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        return gradients, loss, advantage\n",
    "    \n",
    " # *************************************************MODIFIED**************************************************     \n",
    "    def get_actor_grads(self, states, actions, advantage):  # parameters are updated to maximize the expected cumulative reward, incorporating feedback from the critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            # get distribution from current policy (used to sample/ explore in enviroment)\n",
    "            means_current, log_stds_current = self.actor(states)    # mean and log tensors shape(batchsize,1) wit batchsize = len(states)\n",
    "            normal_dist_current = tfp.distributions.Normal(loc=means_current, scale=tf.exp(log_stds_current))\n",
    "            # sample made actions from the approximated current distribution -> introduces more exploration in the action space by sampling specific actions rather than the whole distribution (mean and std)\n",
    "            p_current = normal_dist_current.prob(actions)\n",
    "            \n",
    "            # get distribution from old policy (used to evaluate current policy in ratio)\n",
    "            means_old, log_stds_old = self.actor_old(states)    # mean and log tensors shape(batchsize,1) wit batchsize = len(states)\n",
    "            normal_dist_old = tfp.distributions.Normal(loc=means_old, scale=tf.exp(log_stds_old))\n",
    "            # sample random actions from the approximated current distribution -> introduces more exploration in the action space by sampling actions rather than selecting the mean directly\n",
    "            p_old = normal_dist_old.prob(actions)\n",
    "\n",
    "            # calculate the ratio to weight the advantage estimate from the critic network (value-based)\n",
    "            ratio = p_current / p_old  #p_current, p_old, ratio tensors of shape(batchsize, batchsize, 1) -> like discrete implementation\n",
    "# ***********************************************************************************************************     \n",
    "\n",
    "            clip_ratio = tf.clip_by_value(ratio, 1-self.epsilon, 1+self.epsilon)\n",
    "            # entropy loss hatten wir probiert, bringt aber wenig --> sollte eigentlich exploration förndern\n",
    "            # standardize advantage\n",
    "            advantage = (advantage - tf.reduce_mean(advantage)) / (tf.keras.backend.std(advantage) + 1e-8)\n",
    "            objective = ratio * advantage\n",
    "            clip_objective = clip_ratio * advantage\n",
    "            loss = -tf.reduce_mean(tf.where(objective < clip_objective, objective, clip_objective))\n",
    "        gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        return gradients, loss\n",
    "        \n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        critic_grads, critic_loss, advantage = self.get_critic_grads(states, rewards, next_states, dones)\n",
    "        actor_grads, actor_loss = self.get_actor_grads(states, actions, advantage)\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def update_frozen_nets(self):\n",
    "        # TODO: set discount factor  -> not necessary\n",
    "        weights = self.actor.get_weights()\n",
    "        self.actor_old.set_weights(weights)\n",
    "        \n",
    "        weights = self.critic.get_weights()\n",
    "        self.target_critic.set_weights(weights)\n",
    "\n",
    "    def save_models(self, actor_path='actor_weights.h5', critic_path='critic_weights.h5'):\n",
    "        self.actor.save_weights(actor_path)\n",
    "        self.critic.save_weights(critic_path)\n",
    "\n",
    "    def load_models(self, actor_path='actor_weights.h5', critic_path='critic_weights.h5'):\n",
    "        try:\n",
    "            self.actor.load_weights(actor_path)\n",
    "            self.critic.load_weights(critic_path)\n",
    "            print('Model loaded sucessful')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f274993-caca-4c6d-b92a-ba5589d87831",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "PPO is an on-policy method. We allways complete a full episode, record the trajectory and the rewards. We then use these to update our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dba5c53a-7ef2-4876-bf80-1bfc696e0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=1000, render=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            obs, r, termination, truncation, _ = env.step(action)\n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "        #print('episode_return', episode_return)\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a4a1a",
   "metadata": {},
   "source": [
    "## Add a Connection to Tensorboard -> online visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557bc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refers to log data and model data -> below for model data\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'model\\\\default_MountainCar_{datum_uhrzeit}'\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76892352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory model\\default_MountainCar_20240103_222230\\log exists.\n",
      "c:\\#_FESTPLATTE\\06_Studium\\Master_HKA\\Semesterdateien\\Semester 1\\Roboterprogrammierung-Hein\\Projekt\\Coding\\model\\default_MountainCar_20240103_222230\\log\n"
     ]
    }
   ],
   "source": [
    "log_dir = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    print(f\"The directory {log_dir} exists.\")\n",
    "    absolute_path = os.path.abspath(log_dir)\n",
    "    print(absolute_path)\n",
    "else:\n",
    "    print(f\"The directory {log_dir} does not exist.\")\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# define all metrics to log\n",
    "def log_metrics(epoch, total_timesteps, critic_loss, actor_loss, episode_return, actor_learning_rate, critic_learning_rate, epsilon, gamma):\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('epoch', epoch, step=epoch)\n",
    "        tf.summary.scalar('total_timesteps', total_timesteps, step=epoch)\n",
    "        tf.summary.scalar('critic_loss', critic_loss, step=epoch)\n",
    "        tf.summary.scalar('actor_loss', actor_loss, step=epoch)\n",
    "        tf.summary.scalar('episode_return', episode_return, step=epoch)\n",
    "        tf.summary.scalar('critic_learning_rate', critic_learning_rate, step=epoch)\n",
    "        tf.summary.scalar('actor_learning_rate', actor_learning_rate, step=epoch)\n",
    "        tf.summary.scalar('discount_factor_gamma', gamma, step=epoch)\n",
    "        tf.summary.scalar('clip_range_epsilon', epsilon, step=epoch)\n",
    "\n",
    "# tensorboard --logdir log in bash to open the online editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ff7e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')  #human fur pygame gui -> very laggy!\n",
    "\n",
    "agent = PPOAgent(env.action_space, env.observation_space)   # observation_space entpacken für obserbation_shape (siehe __init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5119ca2a-ef65-4676-b51b-41743eb2a9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done =  True\n",
      "total_timesteps =  999\n",
      "rollout =  0\n",
      "epoch =  0\n",
      "start new rollout\n",
      "start new rollout\n",
      "start new rollout\n",
      "start new rollout\n",
      "start new rollout\n",
      "collected experience in rollouts finished, start learning phase\n",
      "update online nets, one learn step finished\n",
      "update online nets, one learn step finished\n",
      "update online nets, one learn step finished\n",
      "update online nets, one learn step finished\n",
      "update online nets, one learn step finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 73\u001b[0m\n\u001b[0;32m     69\u001b[0m         dones_batch \u001b[38;5;241m=\u001b[39m shuffled_dones[j:j \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;66;03m#print('try to call learn method with shuffled data')\u001b[39;00m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;66;03m# push one batch of the shuffled experience to the learning method -> one update of the current nets (actor and critic) per passed batch of experience\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m         actor_loss, critic_loss \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mlearn(states_batch,\n\u001b[0;32m     74\u001b[0m                                               actions_batch,\n\u001b[0;32m     75\u001b[0m                                               rewards_batch,\n\u001b[0;32m     76\u001b[0m                                               next_states_batch,\n\u001b[0;32m     77\u001b[0m                                               dones_batch)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupdate online nets, one learn step finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate_frozen_nets()\n",
      "Cell \u001b[1;32mIn[73], line 126\u001b[0m, in \u001b[0;36mPPOAgent.learn\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\u001b[38;5;28mself\u001b[39m, states, actions, rewards, next_states, dones):\n\u001b[1;32m--> 126\u001b[0m     critic_grads, critic_loss, advantage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_critic_grads(states, rewards, next_states, dones)\n\u001b[0;32m    127\u001b[0m     actor_grads, actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_actor_grads(states, actions, advantage)\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(critic_grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mtrainable_variables))\n",
      "Cell \u001b[1;32mIn[73], line 92\u001b[0m, in \u001b[0;36mPPOAgent.get_critic_grads\u001b[1;34m(self, states, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m     90\u001b[0m     advantage \u001b[38;5;241m=\u001b[39m q_value \u001b[38;5;241m-\u001b[39m value\n\u001b[0;32m     91\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39msquare(advantage))\n\u001b[1;32m---> 92\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradients, loss, advantage\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1062\u001b[0m           output_gradients))\n\u001b[0;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m imperative_grad\u001b[38;5;241m.\u001b[39mimperative_grad(\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape,\n\u001b[0;32m   1068\u001b[0m     flat_targets,\n\u001b[0;32m   1069\u001b[0m     flat_sources,\n\u001b[0;32m   1070\u001b[0m     output_gradients\u001b[38;5;241m=\u001b[39moutput_gradients,\n\u001b[0;32m   1071\u001b[0m     sources_raw\u001b[38;5;241m=\u001b[39mflat_sources_raw,\n\u001b[0;32m   1072\u001b[0m     unconnected_gradients\u001b[38;5;241m=\u001b[39munconnected_gradients)\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[38;5;241m.\u001b[39m_tape,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str(unconnected_gradients\u001b[38;5;241m.\u001b[39mvalue))\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:118\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    109\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_control_flow_context\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.GradientTape.gradients() does not support graph control flow \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperations like tf.cond or tf.while at this time. Use tf.gradients() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead. If you need this feature, please file a feature request at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/tensorflow/tensorflow/issues/new\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m     )\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gradient_function\u001b[39m(op_name, attr_tuple, num_inputs, inputs, outputs,\n\u001b[0;32m    119\u001b[0m                        out_grads, skip_input_indices, forward_pass_name_scope):\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the gradient function of the op.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    The gradients with respect to the inputs of the function, as a list.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m    136\u001b[0m   mock_op \u001b[38;5;241m=\u001b[39m _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_rollouts = 5\n",
    "batch_size = 8\n",
    "learn_steps = 16\n",
    "total_timesteps = 0\n",
    "\n",
    "for epoch in range(150):    # one epoch -> one complete learning iteration bzw. one update of the frozen nets\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    \n",
    "    # collect experience in the enviroment with current policy for n episodes/ rollouts\n",
    "    # give the agent more time to collect experiences more apart from the starting state\n",
    "    for rollout in range(n_rollouts):\n",
    "        obs, _ = env.reset()\n",
    "        done = False \n",
    "                \n",
    "        while not done:\n",
    "            env.render()    # call gui if render_mode = 'human'\n",
    "            action = agent.act(np.array([obs]))\n",
    "            new_obs, r, termination, truncation, _ = env.step(action)\n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "\n",
    "            states.append(obs)\n",
    "            rewards.append([r])\n",
    "            actions.append([action])\n",
    "            obs = new_obs\n",
    "            next_states.append(obs)\n",
    "            dones.append([done])\n",
    "\n",
    "            total_timesteps += 1\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(env.render())\n",
    "            plt.show()\n",
    "            print(\"done = \", done)\n",
    "            print(\"total_timesteps = \", total_timesteps)\n",
    "            print(\"rollout = \", rollout)\n",
    "            print(\"epoch = \", epoch)\n",
    "\n",
    "        print('start new rollout')\n",
    "    \n",
    "    # store colledted experience for all rollouts/ episodes and reset enviroment for next episode/ rollout\n",
    "    states, actions, rewards, next_states, dones = map(np.array, [states, actions, rewards, next_states, dones])\n",
    "    obs, _ = env.reset()\n",
    "    print('collected experience in rollouts finished, start learning phase')\n",
    "\n",
    "    # learn policy and value from the collected data \n",
    "    for _ in range(learn_steps):\n",
    "        indices = np.arange(states.shape[0])\n",
    "        np.random.shuffle(indices)  # create random indice row\n",
    "        \n",
    "        # switch indices to random experience distribution\n",
    "        shuffled_states = states[indices]\n",
    "        shuffled_actions = actions[indices]\n",
    "        shuffled_rewards = rewards[indices]\n",
    "        shuffled_next_states = next_states[indices]\n",
    "        shuffled_dones = dones[indices]\n",
    "\n",
    "        # divides the whole shuffled experience into batches of batch_size\n",
    "        for j in range(0, states.shape[0], batch_size):\n",
    "            states_batch = shuffled_states[j:j + batch_size]    # j:j + batch_size -> returns all elements from x*batch_size to (x+1)*batch_size\n",
    "            actions_batch = shuffled_actions[j:j + batch_size]\n",
    "            rewards_batch = shuffled_rewards[j:j + batch_size]\n",
    "            next_states_batch = shuffled_next_states[j:j + batch_size]\n",
    "            dones_batch = shuffled_dones[j:j + batch_size]\n",
    "            \n",
    "            #print('try to call learn method with shuffled data')\n",
    "            # push one batch of the shuffled experience to the learning method -> one update of the current nets (actor and critic) per passed batch of experience\n",
    "            actor_loss, critic_loss = agent.learn(states_batch,\n",
    "                                                  actions_batch,\n",
    "                                                  rewards_batch,\n",
    "                                                  next_states_batch,\n",
    "                                                  dones_batch)\n",
    "        print('update online nets, one learn step finished')\n",
    "\n",
    "    agent.update_frozen_nets()\n",
    "    print('update frozen nets, one learn episode finished')\n",
    "\n",
    "    one_episode_return = compute_avg_return(env, agent, num_episodes=1)\n",
    "\n",
    "    #do some more prints for analyzing the model behavior while training\n",
    "    print(f'epoch {epoch + 1}, total_timesteps {total_timesteps}, actor loss {actor_loss}, critic loss {critic_loss}, avg_return {one_episode_return}')\n",
    "    \n",
    "    # Log metrics at the end of each epoch to tensorboard\n",
    "    log_metrics(epoch = epoch, total_timesteps = total_timesteps,\n",
    "                critic_loss = critic_loss, actor_loss= actor_loss,\n",
    "                episode_return = one_episode_return,\n",
    "                actor_learning_rate = agent.actor_learning_rate, critic_learning_rate = agent.critic_learning_rate,\n",
    "                epsilon = agent.epsilon, gamma = agent.gamma)\n",
    "    \n",
    "    # do prints every 10 epochs\n",
    "    # if (i + 1) % 10 == 0:\n",
    "    #     avg_return = compute_avg_return(env, agent, num_episodes=1)\n",
    "    #     print(f'epoch {i + 1}, total_timesteps {total_timesteps}, actor loss {actor_loss}, critic loss {critic_loss}, avg_return {avg_return}')\n",
    "    \n",
    "env.close() # kill gui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db997f26",
   "metadata": {},
   "source": [
    "# Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f418f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to h5 format\n",
    "#filepath_actor = f\"model/default_CartPole_{datum_uhrzeit}/actor.h5\"\n",
    "filepath_actor = f\"{savedir}\\\\actor.h5\"\n",
    "\n",
    "#filepath_critic = f\"model/default_CartPole_{datum_uhrzeit}/critic.h5\"\n",
    "filepath_critic = f\"{savedir}\\\\critic.h5\"\n",
    "\n",
    "agent.save_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27870f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 2] Unable to open file (unable to open file: name = 'model/default_CartPole_20240103_222230_actor.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# load the model from h5 format -> use new agent in new instance of the enviroment to prevent overwriting\n",
    "load_env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
    "\n",
    "load_agent = PPOAgent(env.action_space, env.observation_space)\n",
    "load_agent._init_networks()\n",
    "\n",
    "filepath_actor = f\"model/default_CartPole_{datum_uhrzeit}_actor.h5\"\n",
    "filepath_critic = f\"model/default_CartPole_{datum_uhrzeit}_critic.h5\"\n",
    "\n",
    "load_agent.load_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf9fff65-511c-46e2-b01c-0290e7f0548e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_avg_return(env, agent, num_episodes=20, render=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
