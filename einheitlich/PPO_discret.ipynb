{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc84b80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.callbacks import TensorBoard  # to visualize the training process\n",
    "import os\n",
    "import datetime\n",
    "import pygame\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9daa54a",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9ccbb",
   "metadata": {},
   "source": [
    "\n",
    "PPO is a policy gradient Actor-Critic algorithm. The policy model, the **actor** network  produces a stochastic policy. It maps the state to a probability distribution over the set of possible actions. The **critic** network is used to approximate the value function and then, the advantage is calculated:\n",
    "\n",
    "$$\n",
    "A_\\Phi (s_t, a_t) = q_\\Phi (s_t,a_t) - v_\\Phi (s_t) = R_t + \\gamma v_{\\Phi'} (s_{t+1}) - v_\\Phi (s_t)\n",
    "$$\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$\n",
    "\n",
    "PPO uses 2 main models. The actor network learns the stochastic policy. It maps the state to a probability distribution over the set of possible actions. The critic network learns the value function. It maps the state to a scalar.\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edab71a",
   "metadata": {},
   "source": [
    "## Add a Connection to Tensorboard -> online visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113745e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refers to log data and model data -> below for model data\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'model\\\\MountainCar_discret_{datum_uhrzeit}'\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "504ba380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory model\\MountainCar_discret_20240108_204123\\log exists.\n",
      "c:\\Users\\Mathias\\Documents\\StudiumMaster\\Semester1\\Roboterprogrammierung_Hein\\Projektarbeit_PPO\\02_Code\\model\\MountainCar_discret_20240108_204123\\log\n"
     ]
    }
   ],
   "source": [
    "log_dir1 = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir1, exist_ok=True)\n",
    "\n",
    "if os.path.exists(log_dir1):\n",
    "    print(f\"The directory {log_dir1} exists.\")\n",
    "    absolute_path = os.path.abspath(log_dir1)\n",
    "    print(absolute_path)\n",
    "else:\n",
    "    print(f\"The directory {log_dir1} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25451a8f",
   "metadata": {},
   "source": [
    "## Parameter/ Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b40b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for the actor and critic networks\n",
    "actor_learning_rate = 0.00025   # learning rate for the actor\n",
    "critic_learning_rate = 0.001    # learning rate for the critic\n",
    "\n",
    "# Parameter for the agent\n",
    "gamma = 0.99                    # discount factor\n",
    "epsilon = 0.1                   # clip range for the actor loss function\n",
    "\n",
    "# Parameter for training\n",
    "epochs = 1                   # number of learning iterations\n",
    "n_rollouts = 1#5                  # number of episodes/ rollouts to collect experience\n",
    "batch_size = 8                  # number of samples per learning step\n",
    "learn_steps = 1#16                # number of learning steps per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94424f8",
   "metadata": {},
   "source": [
    "## Environment initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1efba0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomMtnCarEnvironments import CustomMountainCarEnv_acceleration\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')  #human fur pygame gui -> very laggy!\n",
    "env = CustomMountainCarEnv_acceleration(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf789c",
   "metadata": {},
   "source": [
    "## PPO-Agent initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfe25063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PPOAgentDiscrete import PPOAgentDiscrete as PPOAgent\n",
    "agent = PPOAgent(env.action_space, env.observation_space, gamma, epsilon, actor_learning_rate, critic_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3da690",
   "metadata": {},
   "source": [
    "## PPO-Agent trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f009411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, learn step 0 of 1 finished\n",
      "update frozen nets, epoche 0 of 1 finished\n",
      "===> epoch 1, total_timesteps 200, actor loss -0.03959915041923523, critic loss 3.139740467071533, avg_epoch_return 64.0, sum_epoch_terminations 0\n"
     ]
    }
   ],
   "source": [
    "from train_agent import training\n",
    "training(env, agent, log_dir1, epochs, n_rollouts, batch_size, learn_steps, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f274993-caca-4c6d-b92a-ba5589d87831",
   "metadata": {},
   "source": [
    "# Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ff7e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to h5 format\n",
    "filepath_actor = f\"{savedir}\\\\actor.h5\"\n",
    "filepath_critic = f\"{savedir}\\\\critic.h5\"\n",
    "agent.save_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff6a45d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded sucessful\n"
     ]
    }
   ],
   "source": [
    "# load the model from h5 format -> use new agent in new instance of the enviroment to prevent overwriting\n",
    "load_env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
    "\n",
    "load_agent = PPOAgent(env.action_space, env.observation_space)\n",
    "load_agent._init_networks()\n",
    "\n",
    "# filepath_actor = f\"... .h5\"\n",
    "# filepath_critic = f\"... .h5\"\n",
    "\n",
    "load_agent.load_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfdf8ec",
   "metadata": {},
   "source": [
    "## Rendering with pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10110592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded sucessful\n",
      "Episode 0 finished\n",
      "Episode 1 finished\n",
      "Episode 2 finished\n",
      "Closed Rendering sucessful\n"
     ]
    }
   ],
   "source": [
    "from render_GUI import render_GUI\n",
    "\n",
    "\n",
    "# Set up the enviroment and load the trained agent from directory\n",
    "render_env = gym.make('MountainCar-v0', render_mode = 'human')\n",
    "render_agent = PPOAgent(render_env.action_space, render_env.observation_space)\n",
    "\n",
    "# filepath_actor = f\"... .h5\"\n",
    "# filepath_critic = f\"... .h5\"\n",
    "\n",
    "#call the function\n",
    "render_GUI(render_env, render_agent, filepath_actor, filepath_critic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
