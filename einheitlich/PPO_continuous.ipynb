{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b358710c-7fbd-482a-84a8-d220cda6de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "#import tensorflow_probability as tfp\n",
    "from keras.callbacks import TensorBoard\n",
    "#from IPython.display import clear_output\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import pygame\n",
    "import mujoco\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b3b28",
   "metadata": {},
   "source": [
    "## TODO erkl채ren was f체r continuous ge채ndert werden musste"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73375a4",
   "metadata": {},
   "source": [
    "## Add a Connection to Tensorboard -> online visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadbbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refers to log data and model data -> below for model data\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'model\\Hopper_standartsampling_{datum_uhrzeit}'\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a63216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory model\\Hopper_standartsampling_20240108_210555\\log exists.\n",
      "c:\\#_FESTPLATTE\\06_Studium\\Master_HKA\\Semesterdateien\\Semester 1\\Roboterprogrammierung-Hein\\Projekt\\Coding\\einheitlich\\model\\Hopper_standartsampling_20240108_210555\\log\n"
     ]
    }
   ],
   "source": [
    "#from Fctn_log_metrics import log_metrics\n",
    "log_dir1 = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir1, exist_ok=True)\n",
    "\n",
    "if os.path.exists(log_dir1):\n",
    "    print(f\"The directory {log_dir1} exists.\")\n",
    "    absolute_path = os.path.abspath(log_dir1)\n",
    "    print(absolute_path)\n",
    "else:\n",
    "    print(f\"The directory {log_dir1} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71cf6b",
   "metadata": {},
   "source": [
    "## Parameter/ Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991b4e3",
   "metadata": {},
   "source": [
    "MountainCar Continuous: max 1000 steps per Episode\n",
    "Hopper: max 1000 steps per Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8f2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter for the actor and critic networks\n",
    "actor_learning_rate = 0.0001   # learning rate for the actor -----------> changed due to nan issue while training\n",
    "critic_learning_rate = 0.001    # learning rate for the critic\n",
    "\n",
    "# Parameter for the agent\n",
    "gamma = 0.99                    # discount factor\n",
    "epsilon = 0.1                   # clip range for the actor loss function\n",
    "\n",
    "# Parameter for training\n",
    "epochs =  300                  # number of learning iterations\n",
    "n_rollouts = 5                  # number of episodes/ rollouts to collect experience\n",
    "batch_size = 8                  # number of samples per learning step\n",
    "learn_steps = 16                # number of learning steps per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b71200",
   "metadata": {},
   "source": [
    "## Environment initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff7e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomMtnCarEnvironments import CustomMountainCarEnv_acceleration\n",
    "\n",
    "#env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')  #human fur pygame gui -> very laggy!\n",
    "#env = CustomMountainCarEnv_acceleration(env)\n",
    "\n",
    "env = gym.make('Hopper-v4', render_mode='rgb_array')  #human fur pygame gui -> slows down training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f541dfc",
   "metadata": {},
   "source": [
    "## PPO-Agent initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc6109e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PPOAgentContinuous import PPOAgentContinuous as PPOAgent\n",
    "agent = PPOAgent(env.action_space, env.observation_space, gamma, epsilon, actor_learning_rate, critic_learning_rate)   # observation_space entpacken f체r obserbation_shape (siehe __init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48083f6d",
   "metadata": {},
   "source": [
    "## PPO-Agent trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcea69d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, learn step 0 of 16 finished\n",
      "update online nets, learn step 1 of 16 finished\n",
      "update online nets, learn step 2 of 16 finished\n",
      "update online nets, learn step 3 of 16 finished\n",
      "update online nets, learn step 4 of 16 finished\n",
      "update online nets, learn step 5 of 16 finished\n",
      "update online nets, learn step 6 of 16 finished\n",
      "update online nets, learn step 7 of 16 finished\n",
      "update online nets, learn step 8 of 16 finished\n",
      "update online nets, learn step 9 of 16 finished\n",
      "update online nets, learn step 10 of 16 finished\n",
      "update online nets, learn step 11 of 16 finished\n",
      "update online nets, learn step 12 of 16 finished\n",
      "update online nets, learn step 13 of 16 finished\n",
      "update online nets, learn step 14 of 16 finished\n",
      "update online nets, learn step 15 of 16 finished\n",
      "update frozen nets, epoche 0 of 300 finished\n",
      "===> epoch 1, total_timesteps 98, actor loss 0.00414411211386323, critic loss 0.0005242784973233938, avg_epoch_return 11.005287594208964, sum_epoch_terminations 5\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, learn step 0 of 16 finished\n",
      "update online nets, learn step 1 of 16 finished\n",
      "update online nets, learn step 2 of 16 finished\n",
      "update online nets, learn step 3 of 16 finished\n",
      "update online nets, learn step 4 of 16 finished\n",
      "update online nets, learn step 5 of 16 finished\n",
      "update online nets, learn step 6 of 16 finished\n",
      "update online nets, learn step 7 of 16 finished\n",
      "update online nets, learn step 8 of 16 finished\n",
      "update online nets, learn step 9 of 16 finished\n",
      "update online nets, learn step 10 of 16 finished\n",
      "update online nets, learn step 11 of 16 finished\n",
      "update online nets, learn step 12 of 16 finished\n",
      "update online nets, learn step 13 of 16 finished\n",
      "update online nets, learn step 14 of 16 finished\n",
      "update online nets, learn step 15 of 16 finished\n",
      "update frozen nets, epoche 1 of 300 finished\n",
      "===> epoch 2, total_timesteps 172, actor loss 0.002228389261290431, critic loss 0.03712722659111023, avg_epoch_return 7.175978349762464, sum_epoch_terminations 5\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, learn step 0 of 16 finished\n",
      "update online nets, learn step 1 of 16 finished\n",
      "update online nets, learn step 2 of 16 finished\n",
      "update online nets, learn step 3 of 16 finished\n",
      "update online nets, learn step 4 of 16 finished\n",
      "update online nets, learn step 5 of 16 finished\n",
      "update online nets, learn step 6 of 16 finished\n",
      "update online nets, learn step 7 of 16 finished\n",
      "update online nets, learn step 8 of 16 finished\n",
      "update online nets, learn step 9 of 16 finished\n",
      "update online nets, learn step 10 of 16 finished\n",
      "update online nets, learn step 11 of 16 finished\n",
      "update online nets, learn step 12 of 16 finished\n",
      "update online nets, learn step 13 of 16 finished\n",
      "update online nets, learn step 14 of 16 finished\n",
      "update online nets, learn step 15 of 16 finished\n",
      "update frozen nets, epoche 2 of 300 finished\n",
      "===> epoch 3, total_timesteps 238, actor loss 0.09941140562295914, critic loss 0.003828998189419508, avg_epoch_return 8.335155542408378, sum_epoch_terminations 5\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, learn step 0 of 16 finished\n",
      "update online nets, learn step 1 of 16 finished\n",
      "update online nets, learn step 2 of 16 finished\n",
      "update online nets, learn step 3 of 16 finished\n",
      "update online nets, learn step 4 of 16 finished\n",
      "update online nets, learn step 5 of 16 finished\n",
      "update online nets, learn step 6 of 16 finished\n",
      "update online nets, learn step 7 of 16 finished\n",
      "update online nets, learn step 8 of 16 finished\n",
      "update online nets, learn step 9 of 16 finished\n",
      "update online nets, learn step 10 of 16 finished\n",
      "update online nets, learn step 11 of 16 finished\n",
      "update online nets, learn step 12 of 16 finished\n",
      "update online nets, learn step 13 of 16 finished\n"
     ]
    }
   ],
   "source": [
    "from train_agent import training\n",
    "training(env, agent, log_dir1, epochs, n_rollouts, batch_size, learn_steps, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db997f26",
   "metadata": {},
   "source": [
    "## Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f418f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to h5 format\n",
    "filepath_actor = f\"{savedir}\\\\actor.h5\"\n",
    "filepath_critic = f\"{savedir}\\\\critic.h5\"\n",
    "agent.save_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27870f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded sucessful\n"
     ]
    }
   ],
   "source": [
    "# load the model from h5 format -> use new agent in new instance of the enviroment to prevent overwriting\n",
    "load_env = gym.make(\"MountainCarContinuous-v0\", render_mode='rgb_array')\n",
    "\n",
    "load_agent = PPOAgent(env.action_space, env.observation_space)\n",
    "load_agent._init_networks()\n",
    "\n",
    "# filepath_actor = f\"... .h5\"\n",
    "# filepath_critic = f\"... .h5\"\n",
    "\n",
    "load_agent.load_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6dbba9",
   "metadata": {},
   "source": [
    "## Rendering with pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99a3107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded sucessful\n",
      "Episode 0 finished\n",
      "Episode 1 finished\n",
      "Episode 2 finished\n",
      "Closed Rendering sucessful\n"
     ]
    }
   ],
   "source": [
    "from render_GUI import render_GUI\n",
    "\n",
    "# Set up the enviroment and load the trained agent from directory\n",
    "render_env = gym.make('MountainCarContinuous-v0', render_mode = 'human')\n",
    "render_agent = PPOAgent(render_env.action_space, render_env.observation_space)\n",
    "\n",
    "# filepath_actor = f\"model/default_MountainCarContinuous_20240104_141521/actor.h5\"\n",
    "# filepath_critic = f\"model/default_MountainCarContinuous_20240104_141521/critic.h5\"\n",
    "\n",
    "#call the function\n",
    "render_GUI(render_env, render_agent, filepath_actor, filepath_critic)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
