{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b358710c-7fbd-482a-84a8-d220cda6de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041b7af-9a74-45db-b58b-d4433987d60f",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "PPO is a policy gradient Actor-Critic algorithm. The policy model, the **actor** network  produces a stochastic policy. It maps the state to a probability distribution over the set of possible actions. The **critic** network is used to approximate the value function and then, the advantage is calculated:\n",
    "\n",
    "$$\n",
    "A_\\Phi (s_t, a_t) = q_\\Phi (s_t,a_t) - v_\\Phi (s_t) = R_t + \\gamma v_{\\Phi'} (s_{t+1}) - v_\\Phi (s_t)\n",
    "$$\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b5d89-2006-432e-a7fd-9c6eb930c5b0",
   "metadata": {},
   "source": [
    "PPO uses 2 main models. The actor network learns the stochastic policy. It maps the state to a probability distribution over the set of possible actions. The critic network learns the value function. It maps the state to a scalar.\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9179d247-1c6d-4307-bea3-def396311e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=(400, 300), n_actions=2, **kwargs):\n",
    "        super(Actor, self).__init__(**kwargs)\n",
    "        self.layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self.layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self.layers.append(tf.keras.layers.Dense(n_actions, activation='softmax'))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self.layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs  #Anzahl der Actions -> Wahrschinlichkeitsverteilung\n",
    "    \n",
    "class Critic(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=(400, 300), **kwargs):\n",
    "        super(Critic, self).__init__(**kwargs)\n",
    "        self.layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self.layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self.layers.append(tf.keras.layers.Dense(1))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self.layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs  #value des aktuellen states\n",
    "    \n",
    "class PPOAgent:\n",
    "    def __init__(self, action_space, observation_space, gamma=0.99, epsilon = 0.2):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space    #added for passing value to act method\n",
    "        self.observation_shape = observation_space.shape[0]\n",
    "        self.actor = Actor(n_actions=action_space.n)\n",
    "        self.actor_old = Actor(n_actions=action_space.n)\n",
    "        self.critic = Critic()\n",
    "        self.target_critic = Critic()\n",
    "        \n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00025)#learning_rate=0.00025\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
    "        \n",
    "    def _init_networks(self):\n",
    "\n",
    "        rand = np.random.rand(1,self.observation_shape)  #generate random normalized numbers (sum = 1, shape = (1,obs_shape))\n",
    "        normalized_rand = rand/np.sum(rand)\n",
    "\n",
    "        initializer_critic = np.zeros([1, self.observation_shape])\n",
    "        # initializer_actor = np.zeros([1, self.observation_shape]) #random.rand\n",
    "        initializer_actor = normalized_rand\n",
    "\n",
    "        test = self.actor(initializer_actor)\n",
    "        #print('initial_probs', test)\n",
    "        self.actor_old(initializer_actor)\n",
    "        \n",
    "        self.critic(initializer_critic)\n",
    "        self.target_critic(initializer_critic)\n",
    "        \n",
    "        self.update_frozen_nets()   #after each 10 episodes (like calc_current_avg)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        probs = self.actor(observation).numpy() #converts observation to np.array\n",
    "        probs = np.squeeze(probs)   #squeezes probs (stochastic policy!!!) to the dataformat passed to np.random -> probs for action:  [[0.3276569  0.3381462  0.33419695]] probs for action squeezed:  [0.3276569  0.3381462  0.33419695]\n",
    "        #print('probs for action squeezed: ', probs)\n",
    "        action = np.random.choice(self.action_space.n, p=probs)  #do action with given policy from last episode (explore/ exploit with actor NN)\n",
    "        #print('choosen action: ', action)\n",
    "        return action\n",
    "    \n",
    "    def get_critic_grads(self, states, next_states, dones, rewards): # similar to dqn\n",
    "        with tf.GradientTape() as tape: # TensorFlow API that is used for automatic differentiation, computing the derivatives of a function with respect to its inputs\n",
    "            # namechange is active for the following block\n",
    "\n",
    "            next_value = self.target_critic(next_states)\n",
    "            q_value = rewards + (1-dones) * self.gamma * next_value #1-dones, damit letzter value = 0 wird (gibt ja kein Folgezustand mehr)\n",
    "            value = self.critic(states) #value, next_value, q-value, advantage are tf.Tensors: shape=(batchsize bzw. samples in this episode, 1), dtype=float32\n",
    "\n",
    "            # calc advantage\n",
    "            advantage = q_value - value # Maß dafür, wie gut die actions in dieser episode (q-value) waren, im Vergleich zu den erwarteten values (die implizit aus der policy hervorgehen?)\n",
    "            # adv > 0: actions besser als erwartet -> actor network/ policy sollte mit der expericnce aus der episode angepasst werden!\n",
    "            advantage = (advantage - tf.reduce_mean(advantage)) / (tf.keras.backend.std(advantage) + 1e-8)  # effectively standardizing the values to have a mean of 0 and a standard deviation of 1\n",
    "            # Loss and optimisation of critic network with NOT standardized average\n",
    "            # Reduces input_tensor along the dimensions given in axis by computing the mean of elements across the dimensions in axis\n",
    "            loss = tf.reduce_mean(tf.square(advantage)) #MSE, datatype tf.Tensor(6.919476, shape=(), dtype=float32 -> 1 loss calculation for each episode\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.critic.trainable_variables)    #calc updated weights for online critic\n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]  \n",
    "        return gradients, loss, advantage\n",
    "    \n",
    "    def get_actor_grads(self, states, actions, advantage):  #calc probs of baseline and actual policy -> calc ratio\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # calc stochastic policy (W-Verteilung über alle actions -> ges. = 1) with critic and critic_old -> updated every %x episodes\n",
    "            p_current = tf.gather(self.actor(states), actions, axis=1)  #tf.gather wird verwendet, um die Wahrscheinlichkeiten der ausgewählten Aktionen (actions) aus der berechneten Policy zu extrahieren.\n",
    "            p_old = tf.gather(self.actor_old(states), actions, axis=1)  #p_current and p_old are tf.tensor datatypes\n",
    "            # p_old, p_current are tf.tensors shape(batch_size x batch_size) -> rows: states during exploration (batch-size), collumns: actions taken from this state with given probability\n",
    "            # one row: one episode with batch_size steps, selected probability for the actions_vector\n",
    "            \n",
    "            # calculate ratio -> factor to evaluate if the current policy optimisation was better (>1) or worse (<1) as the old before update the frozen NN actor old\n",
    "            ratio = p_current / p_old\n",
    "\n",
    "            # do clipping of ratio\n",
    "            clip_ratio = tf.clip_by_value(ratio, 1-self.epsilon, 1+self.epsilon)\n",
    "            objective = ratio * advantage   #(batchsize x batchsize) = (batchsize x batchsize) * (batchsize, 1) -> rowwise multiplication\n",
    "            clip_objective = clip_ratio * advantage\n",
    "\n",
    "            # calculate loss to optimize actor NN (und damit auch stochastic policy)\n",
    "            policy_loss = -tf.reduce_mean(tf.where(objective < clip_objective, objective, clip_objective)) # tf.where: conditional element-wise selection on passed condition\n",
    "            # - to be consistent with the convention in optimization problems, where the objective is typically minimized, but in this case, you are maximizing a negated version of the mean\n",
    "\n",
    "            # ***********************Added Entropy for more diverse Exploration************************* -> penalizes not diverse actions\n",
    "            entropy = -tf.reduce_sum(p_current * tf.math.log(p_current + 1e-10), axis=1, keepdims=True)\n",
    "            entropy_loss = -0.01 * tf.reduce_mean(entropy)  # Adjust the coefficient as needed\n",
    "            total_loss = policy_loss + entropy_loss\n",
    "        gradients = tape.gradient(total_loss, self.actor.trainable_variables) # loss derivated by trainable_variables -> in our case weights phi -> get gradient!\n",
    "\n",
    "        # clip gradients \n",
    "        gradients = [tf.clip_by_value(grad, -1.0, 1.0) for grad in gradients]   # hat normalerweise keinen einfluss da Grads kleiner\n",
    "        return gradients, policy_loss\n",
    "        \n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        # process the networks with ganied experience in one episode\n",
    "        critic_grads, critic_loss, advantage = self.get_critic_grads(states, next_states, dones, rewards)    #process the networks and calculate losses\n",
    "        actor_grads, actor_loss = self.get_actor_grads(states, actions, advantage)  #outputs are arrays! -> one row per step in enviroment during episode experience gathering\n",
    "        \n",
    "        # optimize actor and critic (current!) with calculated loss (bzw. loss schon auf gradients angewendet -> siehe get_grads methoden) with apply:grads method from adams optimizer\n",
    "        # zip: erzeugt Tupel, die aus einem Gradienten und der entsprechenden trainierbaren Variable bestehen --> hier Liste davon, für jedes Gewicht ein Tupel\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))   # update weights with every call (only online nets!) // backpropagation\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))  # adam optimizer is used, see above\n",
    "        \n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def update_frozen_nets(self):   # after each 50 episodes see below\n",
    "# TODO: set discount factor --> muss das überhaupt umgesetzt werden?\n",
    "        weights = self.actor.get_weights()\n",
    "        self.actor_old.set_weights(weights)\n",
    "        weights = self.critic.get_weights()\n",
    "        self.target_critic.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f274993-caca-4c6d-b92a-ba5589d87831",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "PPO is an on-policy method. We allways complete a full episode, record the trajectory and the rewards. We then use these to update our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba5c53a-7ef2-4876-bf80-1bfc696e0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=200, render=True):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            obs, r, termination, truncation, _ = env.step(action)\n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "                \n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5119ca2a-ef65-4676-b51b-41743eb2a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode = 'human')\n",
    "\n",
    "agent = PPOAgent(env.action_space, env.observation_space)   #set up the agent (and both NN)\n",
    "\n",
    "agent._init_networks()\n",
    "\n",
    "for i in range(501):   #one cycle = one episode -> collecting experience till termination and update NN with gathered experience\n",
    "    obs, _ = env.reset()    #reset enviroment -> start new episode\n",
    "    done = False\n",
    "    \n",
    "    # init empty lists for storing experience for one episode\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    \n",
    "    # steps = 0\n",
    "    while not done: #collect experience in episode\n",
    "        \n",
    "        action = agent.act(np.array([obs])) # select action at from current state st by the current policy pi (in this episode!)\n",
    "        #print('obs', obs)\n",
    "        new_obs, r, termination, truncation, info = env.step(action)   # get new state st+1 (after action st)\n",
    "\n",
    "        env.render()\n",
    "\n",
    "        if termination or truncation:\n",
    "            done = True\n",
    "            \n",
    "        states.append(obs)\n",
    "        rewards.append([r])\n",
    "        actions.append(action)\n",
    "        obs = new_obs\n",
    "        next_states.append(obs)\n",
    "        dones.append([done])    # collect episode data by interacting with the enviroment\n",
    "\n",
    "        #rendering live\n",
    "        # clear_output(wait=True)\n",
    "        # plt.axis('off')\n",
    "        # plt.imshow(env.render())\n",
    "        # plt.show()\n",
    "\n",
    "    # store episode date in arrays for processing in learn method -> convert list to np.array\n",
    "    states, actions, rewards, next_states, dones = map(np.array, [states, actions, rewards, next_states, dones])    \n",
    "    actor_loss, critic_loss = agent.learn(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    #avg_return = compute_avg_return(env, agent, num_episodes=2, render = False)\n",
    "    episode_return = np.sum(rewards)\n",
    "    print(f'epoch {i}, actor loss {actor_loss}, critic loss {critic_loss}, episode_return {episode_return}')\n",
    "    \n",
    "    #update frozen nets and print avg returen, loss critic, loss actor\n",
    "    if i % 20 == 0:\n",
    "        agent.update_frozen_nets()  #update frozen nets every 50 episodes\n",
    "        avg_return = compute_avg_return(env, agent, num_episodes=2, render = False)\n",
    "        #print(f'epoch {i}, actor loss {actor_loss}, critic loss {critic_loss}, avg_return {avg_return}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fff65-511c-46e2-b01c-0290e7f0548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "compute_avg_return(env, agent, num_episodes=10, render=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
