{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b358710c-7fbd-482a-84a8-d220cda6de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_probability as tfp\n",
    "from keras.callbacks import TensorBoard\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import pygame\n",
    "import mujoco\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041b7af-9a74-45db-b58b-d4433987d60f",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "PPO is a policy gradient Actor-Critic algorithm. The policy model, the **actor** network  produces a stochastic policy. It maps the state to a probability distribution over the set of possible actions. The **critic** network is used to approximate the value function and then, the advantage is calculated:\n",
    "\n",
    "$$\n",
    "A_\\Phi (s_t, a_t) = q_\\Phi (s_t,a_t) - v_\\Phi (s_t) = R_t + \\gamma v_{\\Phi'} (s_{t+1}) - v_\\Phi (s_t)\n",
    "$$\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b5d89-2006-432e-a7fd-9c6eb930c5b0",
   "metadata": {},
   "source": [
    "PPO uses 2 main models. The actor network learns the stochastic policy. It maps the state to a probability distribution over the set of possible actions. The critic network learns the value function. It maps the state to a scalar.\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9179d247-1c6d-4307-bea3-def396311e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change inheritance class to enable saving the models\n",
    "\n",
    "# *************************************************MODIFIED************************************************** \n",
    "class Actor(tf.keras.Model):    # represents/ approximates the stochastic-policy (policy = weights from the nn)\n",
    "    def __init__(self, units=(400, 300), n_actions=1, **kwargs):    # input = observation shape(batchsize, observation_shape) -> same as in discrete action space \n",
    "        super(Actor, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        n_outputs = n_actions*2 # one continuous output distribution contains values std and mean for gaussian \n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(n_outputs, activation = 'tanh'))   # output = ?? shape(batchsize, n_outputs)\n",
    "        # modify output dimension to n_actions * 2 (= 1 for MountainCarCont) -> output is now std and mean of continuous gaussian distribution\n",
    "        # modify output layer activation function -> use no activation/ linear activation a(x) = x to output the estimated values directly\n",
    "        # if custom clipping is necessary, use tanh as output activation function to clip[-1,1]\n",
    "        # in discrete action space 'softmax' exp(x) / tf.reduce_sum(exp(x)) calculates the value of each output vector in that way, the output can be interpreted as a discrete probability distribution (sum vectors = 1)\n",
    "        \n",
    "    # forward pass through the network\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        # if last layer is reached, prepare the output to return mean and std\n",
    "        mean, log_std = tf.split(outputs, 2, axis=-1)  # Split the output(Tensor shape(batchsize, n_outputs)) into 2 tensors (mean and log_std (ln!)) along the last axis(collums)\n",
    "        #print('mean', mean,'log_std',log_std)\n",
    "        return mean, log_std\n",
    "\n",
    "    \n",
    "class Critic(tf.keras.Model):   # evaluates choosen actions(critic) in reference to the estimated actions(target critic) -> provides feedback to the actor (optipizing the policy was better/ worser)\n",
    "    def __init__(self, units=(400, 300), **kwargs):\n",
    "        super(Critic, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(1))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class PPOAgent:\n",
    "    def __init__(self, action_space, observation_space, gamma=0.99, epsilon = 0.1):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.observation_shape = observation_space.shape[0]\n",
    "# *************************************************MODIFIED**************************************************     \n",
    "        self.action_shape = action_space.shape[0]\n",
    "        self.actor = Actor(n_actions=self.action_shape)\n",
    "        self.actor_old = Actor(n_actions=self.action_shape)\n",
    "# ***********************************************************************************************************     \n",
    "\n",
    "        self.critic = Critic()\n",
    "        self.target_critic = Critic()\n",
    "        \n",
    "        self.actor_learning_rate=0.0001 #0.00025 -> hopper weights explode -> nan output\n",
    "        self.critic_learning_rate=0.001\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(self.actor_learning_rate)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.critic_learning_rate)   # default = 0,001 -> hatten wir auch schon\n",
    "        self._init_networks()\n",
    "        \n",
    "    def _init_networks(self):\n",
    "        initializer = np.zeros([1, self.observation_shape])   # ergänzt zu V1 -> hatten wir aber auch schon gemacht\n",
    "        self.actor(initializer)\n",
    "        self.actor_old(initializer)\n",
    "        \n",
    "        self.critic(initializer)\n",
    "        self.target_critic(initializer)\n",
    "        \n",
    "        self.update_frozen_nets()\n",
    "\n",
    " # *************************************************MODIFIED**************************************************     \n",
    "    def act(self, observation):\n",
    "        mean_tensor, log_std_tensor = self.actor(observation) # Actor network will output the gaussian probability distribution of actions for the given observation-state\n",
    "        mean = tf.gather(mean_tensor, indices= 0, axis=1).numpy()\n",
    "        std = tf.gather(tf.exp(log_std_tensor), indices= 0, axis=1).numpy()\n",
    "        #print('mean', mean, 'std', std)\n",
    "        action = np.random.normal(size = self.action_shape, loc = mean, scale = std)  # modify sampling method to sample random actions in respect a continous normal distribution\n",
    "        #print('Choosen Action:', action)\n",
    "        return action\n",
    "        # in continous action space this output should be a real number (optional clipped [-1,1])\n",
    "        # for mnt car = force applied on the car (clipped [-1,1]) and * power od 0.0015 -> no custom action clipping necessary\n",
    " # ***********************************************************************************************************     \n",
    "   \n",
    "    def get_critic_grads(self, states, rewards, next_states, dones):    # parameters are adjusted to minimize the difference between predicted values and observed returns\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_value = self.target_critic(next_states)\n",
    "            q_value = rewards + (1-dones) * self.gamma * next_value\n",
    "            value = self.critic(states)\n",
    "            \n",
    "            advantage = q_value - value\n",
    "            loss = tf.reduce_mean(tf.square(advantage))\n",
    "        gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        return gradients, loss, advantage\n",
    "    \n",
    " # *************************************************MODIFIED**************************************************     \n",
    "    def get_actor_grads(self, states, actions, advantage):  # parameters are updated to maximize the expected cumulative reward, incorporating feedback from the critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            # get distribution from current policy (used to sample/ explore in enviroment)\n",
    "            means_current, log_stds_current = self.actor(states)    # mean and log tensors shape(batchsize,1) wit batchsize = len(states)\n",
    "            normal_dist_current = tfp.distributions.Normal(loc=means_current, scale=tf.exp(log_stds_current))\n",
    "            # sample made actions from the approximated current distribution -> introduces more exploration in the action space by sampling specific actions rather than the whole distribution (mean and std)\n",
    "            p_current = normal_dist_current.prob(actions)\n",
    "            \n",
    "            # get distribution from old policy (used to evaluate current policy in ratio)\n",
    "            means_old, log_stds_old = self.actor_old(states)    # mean and log tensors shape(batchsize,1) wit batchsize = len(states)\n",
    "            normal_dist_old = tfp.distributions.Normal(loc=means_old, scale=tf.exp(log_stds_old))\n",
    "            # sample random actions from the approximated current distribution -> introduces more exploration in the action space by sampling actions rather than selecting the mean directly\n",
    "            p_old = normal_dist_old.prob(actions)\n",
    "\n",
    "            # calculate the ratio to weight the advantage estimate from the critic network (value-based)\n",
    "            ratio = p_current / p_old  #p_current, p_old, ratio tensors of shape(batchsize, batchsize, 1) -> like discrete implementation\n",
    "# ***********************************************************************************************************     \n",
    "\n",
    "            clip_ratio = tf.clip_by_value(ratio, 1-self.epsilon, 1+self.epsilon)\n",
    "            # entropy loss hatten wir probiert, bringt aber wenig --> sollte eigentlich exploration förndern\n",
    "            # standardize advantage\n",
    "            advantage = (advantage - tf.reduce_mean(advantage)) / (tf.keras.backend.std(advantage) + 1e-8)\n",
    "            objective = ratio * advantage\n",
    "            clip_objective = clip_ratio * advantage\n",
    "            loss = -tf.reduce_mean(tf.where(objective < clip_objective, objective, clip_objective))\n",
    "        gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        return gradients, loss\n",
    "        \n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        critic_grads, critic_loss, advantage = self.get_critic_grads(states, rewards, next_states, dones)\n",
    "        actor_grads, actor_loss = self.get_actor_grads(states, actions, advantage)\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def update_frozen_nets(self):\n",
    "        # TODO: set discount factor  -> not necessary\n",
    "        weights = self.actor.get_weights()\n",
    "        self.actor_old.set_weights(weights)\n",
    "        \n",
    "        weights = self.critic.get_weights()\n",
    "        self.target_critic.set_weights(weights)\n",
    "\n",
    "    def save_models(self, actor_path='actor_weights.h5', critic_path='critic_weights.h5'):\n",
    "        self.actor.save_weights(actor_path)\n",
    "        self.critic.save_weights(critic_path)\n",
    "\n",
    "    def load_models(self, actor_path='actor_weights.h5', critic_path='critic_weights.h5'):\n",
    "        try:\n",
    "            self.actor.load_weights(actor_path)\n",
    "            self.critic.load_weights(critic_path)\n",
    "            print('Model reloaded sucessful')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f274993-caca-4c6d-b92a-ba5589d87831",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "PPO is an on-policy method. We allways complete a full episode, record the trajectory and the rewards. We then use these to update our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba5c53a-7ef2-4876-bf80-1bfc696e0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=1000, render=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            obs, r, termination, truncation, _ = env.step(action)\n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "        #print('episode_return', episode_return)\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a4a1a",
   "metadata": {},
   "source": [
    "## Add a Connection to Tensorboard -> online visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557bc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refers to log data and model data -> below for model data\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'model\\\\please_delete{datum_uhrzeit}'\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76892352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory model\\please_delete20240107_181043\\log exists.\n",
      "c:\\#_FESTPLATTE\\06_Studium\\Master_HKA\\Semesterdateien\\Semester 1\\Roboterprogrammierung-Hein\\Projekt\\Coding\\model\\please_delete20240107_181043\\log\n"
     ]
    }
   ],
   "source": [
    "log_dir = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    print(f\"The directory {log_dir} exists.\")\n",
    "    absolute_path = os.path.abspath(log_dir)\n",
    "    print(absolute_path)\n",
    "else:\n",
    "    print(f\"The directory {log_dir} does not exist.\")\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# define all metrics to log\n",
    "def log_metrics(epoch, terminations, total_timesteps, critic_loss, actor_loss, episode_return, actor_learning_rate, critic_learning_rate, epsilon, gamma):\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('epoch', epoch, step=epoch)\n",
    "        tf.summary.scalar('terminations per epoch', terminations, step=epoch)\n",
    "        tf.summary.scalar('total_timesteps', total_timesteps, step=epoch)\n",
    "        tf.summary.scalar('critic_loss', critic_loss, step=epoch)\n",
    "        tf.summary.scalar('actor_loss', actor_loss, step=epoch)\n",
    "        tf.summary.scalar('episode_return', episode_return, step=epoch)\n",
    "        tf.summary.scalar('critic_learning_rate', critic_learning_rate, step=epoch)\n",
    "        tf.summary.scalar('actor_learning_rate', actor_learning_rate, step=epoch)\n",
    "        tf.summary.scalar('discount_factor_gamma', gamma, step=epoch)\n",
    "        tf.summary.scalar('clip_range_epsilon', epsilon, step=epoch)\n",
    "\n",
    "# tensorboard --logdir log in bash to open the online editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ff7e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('MountainCarContinuous-v0', render_mode='rgb_array')  #human fur pygame gui -> very laggy!\n",
    "env = gym.make('Hopper-v4', render_mode='rgb_array')  #human fur pygame gui -> very laggy!\n",
    "\n",
    "agent = PPOAgent(env.action_space, env.observation_space)   # observation_space entpacken für obserbation_shape (siehe __init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5119ca2a-ef65-4676-b51b-41743eb2a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 0 learn episode finished\n",
      "epoch 1, total_timesteps 162, actor loss 0.011115838773548603, critic loss 0.006888529285788536, avg_return 6.877129706478098\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 1 learn episode finished\n",
      "epoch 2, total_timesteps 261, actor loss 0.010844760574400425, critic loss 0.12139347940683365, avg_return 7.542394381187214\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 2 learn episode finished\n",
      "epoch 3, total_timesteps 321, actor loss 0.007464816328138113, critic loss 0.11547823250293732, avg_return 5.631360588183235\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 3 learn episode finished\n",
      "epoch 4, total_timesteps 433, actor loss 0.0030243892688304186, critic loss 0.13480490446090698, avg_return 16.04202131622913\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 4 learn episode finished\n",
      "epoch 5, total_timesteps 601, actor loss -0.019272496923804283, critic loss 1.3146744966506958, avg_return 98.76853445726326\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 5 learn episode finished\n",
      "epoch 6, total_timesteps 778, actor loss -0.0, critic loss 1.7192339897155762, avg_return 9.961107243335409\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 6 learn episode finished\n",
      "epoch 7, total_timesteps 869, actor loss 0.08304385840892792, critic loss 2.097101926803589, avg_return 7.951373724829133\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 7 learn episode finished\n",
      "epoch 8, total_timesteps 1015, actor loss 0.009070376865565777, critic loss 0.10762336105108261, avg_return 10.711577052488833\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 8 learn episode finished\n",
      "epoch 9, total_timesteps 1182, actor loss 0.0008153258822858334, critic loss 1.4341622591018677, avg_return 8.69574974706441\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 9 learn episode finished\n",
      "epoch 10, total_timesteps 1347, actor loss -0.003542067250236869, critic loss 1.9966423511505127, avg_return 18.97104999292529\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 10 learn episode finished\n",
      "epoch 11, total_timesteps 1449, actor loss -0.002142916666343808, critic loss 1.0142375230789185, avg_return 5.276839744082753\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 11 learn episode finished\n",
      "epoch 12, total_timesteps 1554, actor loss -0.0, critic loss 0.09924974292516708, avg_return 9.302275571982197\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 12 learn episode finished\n",
      "epoch 13, total_timesteps 1747, actor loss -0.0, critic loss 0.7364910244941711, avg_return 10.23149258889009\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 13 learn episode finished\n",
      "epoch 14, total_timesteps 1847, actor loss 0.00030211606645025313, critic loss 2.9496681690216064, avg_return 8.927642659797153\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 14 learn episode finished\n",
      "epoch 15, total_timesteps 1961, actor loss -0.005638102535158396, critic loss 3.613323926925659, avg_return 48.4012419874437\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 15 learn episode finished\n",
      "epoch 16, total_timesteps 2180, actor loss -0.0220069270581007, critic loss 6.890889644622803, avg_return 88.77700258679556\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 16 learn episode finished\n",
      "epoch 17, total_timesteps 2294, actor loss 0.008907963521778584, critic loss 15.32129955291748, avg_return 67.40685334681609\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 17 learn episode finished\n",
      "epoch 18, total_timesteps 2429, actor loss -0.009747260250151157, critic loss 5.5791449546813965, avg_return 28.2676073692566\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 18 learn episode finished\n",
      "epoch 19, total_timesteps 2553, actor loss -0.024444198235869408, critic loss 2.6774394512176514, avg_return 56.5233821929489\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 19 learn episode finished\n",
      "epoch 20, total_timesteps 2667, actor loss 0.01641261577606201, critic loss 0.3578179180622101, avg_return 11.973563464799131\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 20 learn episode finished\n",
      "epoch 21, total_timesteps 2804, actor loss -0.0, critic loss 0.05874351039528847, avg_return 9.935550043171835\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 21 learn episode finished\n",
      "epoch 22, total_timesteps 2956, actor loss -0.00911866594105959, critic loss 4.370360374450684, avg_return 5.877499040191024\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 22 learn episode finished\n",
      "epoch 23, total_timesteps 3072, actor loss -0.004779537674039602, critic loss 3.743217945098877, avg_return 9.241819083087458\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 23 learn episode finished\n",
      "epoch 24, total_timesteps 3157, actor loss 0.004503358155488968, critic loss 5.612795829772949, avg_return 51.16075954549884\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 24 learn episode finished\n",
      "epoch 25, total_timesteps 3316, actor loss -0.002208670834079385, critic loss 15.284459114074707, avg_return 11.185112289651606\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 25 learn episode finished\n",
      "epoch 26, total_timesteps 3487, actor loss -0.006356150843203068, critic loss 2.1936817169189453, avg_return 75.85522341416676\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 26 learn episode finished\n",
      "epoch 27, total_timesteps 3626, actor loss 0.0004739297728519887, critic loss 8.272393226623535, avg_return 63.031879577326244\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 27 learn episode finished\n",
      "epoch 28, total_timesteps 3793, actor loss 0.0026321006007492542, critic loss 3.253781795501709, avg_return 47.742313223523006\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 28 learn episode finished\n",
      "epoch 29, total_timesteps 3946, actor loss -0.0, critic loss 0.3248496353626251, avg_return 6.2208085533782125\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 29 learn episode finished\n",
      "epoch 30, total_timesteps 4064, actor loss -0.001784734195098281, critic loss 1.7515687942504883, avg_return 44.08700816868854\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 30 learn episode finished\n",
      "epoch 31, total_timesteps 4199, actor loss 0.0010929456911981106, critic loss 4.83596134185791, avg_return 45.00635832595235\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 31 learn episode finished\n",
      "epoch 32, total_timesteps 4338, actor loss 0.007232939824461937, critic loss 25.406137466430664, avg_return 44.304063811731886\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 32 learn episode finished\n",
      "epoch 33, total_timesteps 4492, actor loss 0.005082070827484131, critic loss 4.582082748413086, avg_return 48.13311147949116\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 33 learn episode finished\n",
      "epoch 34, total_timesteps 4662, actor loss 0.016621559858322144, critic loss 0.4807804524898529, avg_return 24.687472116898558\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 34 learn episode finished\n",
      "epoch 35, total_timesteps 4909, actor loss -0.002358986297622323, critic loss 6.858790397644043, avg_return 44.35002603860568\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 35 learn episode finished\n",
      "epoch 36, total_timesteps 5073, actor loss -0.006765355821698904, critic loss 7.842486381530762, avg_return 10.181423937678725\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 36 learn episode finished\n",
      "epoch 37, total_timesteps 5243, actor loss -0.0025610923767089844, critic loss 14.395909309387207, avg_return 50.60293458799538\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 37 learn episode finished\n",
      "epoch 38, total_timesteps 5392, actor loss -0.02030366286635399, critic loss 5.164393424987793, avg_return 72.3486266200152\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 38 learn episode finished\n",
      "epoch 39, total_timesteps 5576, actor loss -0.003966229502111673, critic loss 1.7804100513458252, avg_return 48.43540387460784\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 39 learn episode finished\n",
      "epoch 40, total_timesteps 5741, actor loss -0.0016551049193367362, critic loss 7.60382604598999, avg_return 50.53920035406689\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 40 learn episode finished\n",
      "epoch 41, total_timesteps 5898, actor loss 0.019447721540927887, critic loss 0.2068963348865509, avg_return 66.15564629610586\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 41 learn episode finished\n",
      "epoch 42, total_timesteps 6044, actor loss 0.014851708896458149, critic loss 1.6314983367919922, avg_return 72.73627835338219\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 42 learn episode finished\n",
      "epoch 43, total_timesteps 6188, actor loss 0.01500198245048523, critic loss 7.476856231689453, avg_return 44.95309265784682\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 43 learn episode finished\n",
      "epoch 44, total_timesteps 6350, actor loss 0.00018912553787231445, critic loss 5.110050678253174, avg_return 50.334926786142894\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 44 learn episode finished\n",
      "epoch 45, total_timesteps 6487, actor loss -0.0, critic loss 8.907541275024414, avg_return 43.28668660287005\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 45 learn episode finished\n",
      "epoch 46, total_timesteps 6635, actor loss 0.04157058894634247, critic loss 0.5378032922744751, avg_return 41.702063508721025\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 46 learn episode finished\n",
      "epoch 47, total_timesteps 6777, actor loss 0.023331452161073685, critic loss 1.9198940992355347, avg_return 47.88815274444454\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 47 learn episode finished\n",
      "epoch 48, total_timesteps 6950, actor loss -0.0060172779485583305, critic loss 1.0274354219436646, avg_return 45.178469240843555\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 48 learn episode finished\n",
      "epoch 49, total_timesteps 7093, actor loss 0.049492496997117996, critic loss 2.25108003616333, avg_return 48.42080345877586\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 49 learn episode finished\n",
      "epoch 50, total_timesteps 7272, actor loss 0.011824925430119038, critic loss 1.5028377771377563, avg_return 46.89495598841762\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 50 learn episode finished\n",
      "epoch 51, total_timesteps 7428, actor loss -0.010689586400985718, critic loss 5.938920974731445, avg_return 45.99410583049528\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 51 learn episode finished\n",
      "epoch 52, total_timesteps 7577, actor loss -0.019297176972031593, critic loss 7.00079345703125, avg_return 51.56369929636823\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 52 learn episode finished\n",
      "epoch 53, total_timesteps 7721, actor loss 0.002263302681967616, critic loss 2.8037259578704834, avg_return 52.515880887440915\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 53 learn episode finished\n",
      "epoch 54, total_timesteps 7867, actor loss 0.006519446615129709, critic loss 0.8507732152938843, avg_return 46.174121416103844\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 54 learn episode finished\n",
      "epoch 55, total_timesteps 8032, actor loss 0.002010615775361657, critic loss 0.2343091517686844, avg_return 55.29265560201639\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n",
      "update online nets, 3 learn step finished\n",
      "update frozen nets, 55 learn episode finished\n",
      "epoch 56, total_timesteps 8303, actor loss 0.06428036838769913, critic loss 2.0655677318573, avg_return 44.95051323078914\n",
      "collecting experience in rollouts finished, start learning phase\n",
      "update online nets, 0 learn step finished\n",
      "update online nets, 1 learn step finished\n",
      "update online nets, 2 learn step finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17920\\4120926018.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mdones_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffled_dones\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mj\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;31m#print('try to call learn method with shuffled data')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;31m# push one batch of the shuffled experience to the learning method -> one update of the current nets (actor and critic) per passed batch of experience\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             actor_loss, critic_loss = agent.learn(states_batch,\n\u001b[0m\u001b[0;32m     73\u001b[0m                                                   \u001b[0mactions_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                                                   \u001b[0mrewards_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                                                   \u001b[0mnext_states_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17920\\387620575.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0mcritic_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_critic_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mactor_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actor_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[0;32m   1219\u001b[0m             \u001b[1;34m\"experimental_aggregate_gradients\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m         )\n\u001b[0;32m   1221\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mskip_gradients_aggregation\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[0;32m    648\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clip_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deduplicate_sparse_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_weight_decay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m             \u001b[0miteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m             \u001b[1;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mesh\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_with_dtensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m             \u001b[1;31m# Skip any usage of strategy logic for DTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_apply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1253\u001b[1;33m         return tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[0;32m   1254\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_apply_gradients_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mThe\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m   \"\"\"\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n\u001b[0;32m     54\u001b[0m         fn, args=args, kwargs=kwargs)\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[0;32m   1341\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1345\u001b[1;33m             distribution.extended.update(\n\u001b[0m\u001b[0;32m   1346\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m             )\n\u001b[0;32m   1348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3009\u001b[0m         _get_default_replica_context()):\n\u001b[0;32m   3010\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   3011\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m   3012\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3013\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3014\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3015\u001b[0m       return self._replica_ctx_update(\n\u001b[0;32m   3016\u001b[0m           var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4080\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4081\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4082\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4083\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4085\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4086\u001b[0m     \u001b[1;31m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4087\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4088\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4089\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4090\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4091\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4092\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_step_xla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1341\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1342\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    237\u001b[0m                 \u001b[1;34m\"`optimizer.build(variables)` with the full list of trainable \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m                 \u001b[1;34m\"variables before the training loop or use legacy optimizer \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m                 \u001b[1;34mf\"`tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             )\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\adam.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, gradient, variable)\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv_hat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;31m# Dense gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m             \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[0mv_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_velocity_hats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvar_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1475\u001b[0m         \u001b[1;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m         \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1477\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1479\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1480\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m         \u001b[1;31m# object that can implement the operator with knowledge of itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[1;31m# and the tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1877\u001b[0m     new_vals = gen_sparse_ops.sparse_dense_cwise_mul(y.indices, y.values,\n\u001b[0;32m   1878\u001b[0m                                                      y.dense_shape, x, name)\n\u001b[0;32m   1879\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1881\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m    \u001b[1;33m*\u001b[0m \u001b[0mInvalidArgumentError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mWhen\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mincompatible\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m   \"\"\"\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6752\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[0;32m   6753\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6754\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6755\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6756\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6757\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6758\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6759\u001b[0m       return mul_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_rollouts = 5\n",
    "batch_size = 8\n",
    "learn_steps = 16\n",
    "total_timesteps = 0\n",
    "\n",
    "for epoch in range(150):    # one epoch -> one complete learning iteration bzw. one update of the frozen nets\n",
    "    \n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    terminations = 0\n",
    "    \n",
    "    # collect experience in the enviroment with current policy for n episodes/ rollouts\n",
    "    # give the agent more time to collect experiences more apart from the starting state\n",
    "    for rollout in range(n_rollouts):\n",
    "        obs, _ = env.reset()\n",
    "        done = False \n",
    "\n",
    "        while not done:\n",
    "            env.render()    # call gui if render_mode = 'human'\n",
    "            action = agent.act(np.array([obs]))\n",
    "            new_obs, r, termination, truncation, _ = env.step(action)\n",
    "            if termination:\n",
    "                terminations += 1\n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "\n",
    "            states.append(obs)\n",
    "            rewards.append([r])\n",
    "            actions.append([action])\n",
    "            obs = new_obs\n",
    "            next_states.append(obs)\n",
    "            dones.append([done])\n",
    "\n",
    "            total_timesteps += 1\n",
    "\n",
    "            # clear_output(wait=True)\n",
    "            # plt.axis('off')\n",
    "            # plt.imshow(env.render())\n",
    "            # plt.show()\n",
    "            # print(\"done = \", done)\n",
    "            # print(\"total_timesteps = \", total_timesteps)\n",
    "            # print(\"rollout = \", rollout)\n",
    "            # print(\"epoch = \", epoch)\n",
    "    \n",
    "    # store colledted experience for all rollouts/ episodes and reset enviroment for next episode/ rollout\n",
    "    states, actions, rewards, next_states, dones = map(np.array, [states, actions, rewards, next_states, dones])\n",
    "    obs, _ = env.reset()\n",
    "    print('collecting experience in rollouts finished, start learning phase')\n",
    "\n",
    "    # learn policy and value from the collected data \n",
    "    for learn_step in range(learn_steps):\n",
    "        indices = np.arange(states.shape[0])\n",
    "        np.random.shuffle(indices)  # create random indice row\n",
    "        \n",
    "        # switch indices to random experience distribution\n",
    "        shuffled_states = states[indices]\n",
    "        shuffled_actions = actions[indices]\n",
    "        shuffled_rewards = rewards[indices]\n",
    "        shuffled_next_states = next_states[indices]\n",
    "        shuffled_dones = dones[indices]\n",
    "\n",
    "        # divides the whole shuffled experience into batches of batch_size\n",
    "        for j in range(0, states.shape[0], batch_size):\n",
    "            states_batch = shuffled_states[j:j + batch_size]    # j:j + batch_size -> returns all elements from x*batch_size to (x+1)*batch_size\n",
    "            actions_batch = shuffled_actions[j:j + batch_size]\n",
    "            rewards_batch = shuffled_rewards[j:j + batch_size]\n",
    "            next_states_batch = shuffled_next_states[j:j + batch_size]\n",
    "            dones_batch = shuffled_dones[j:j + batch_size]\n",
    "            \n",
    "            #print('try to call learn method with shuffled data')\n",
    "            # push one batch of the shuffled experience to the learning method -> one update of the current nets (actor and critic) per passed batch of experience\n",
    "            actor_loss, critic_loss = agent.learn(states_batch,\n",
    "                                                  actions_batch,\n",
    "                                                  rewards_batch,\n",
    "                                                  next_states_batch,\n",
    "                                                  dones_batch)\n",
    "        print(f'update online nets, {learn_step} learn step finished')\n",
    "\n",
    "    agent.update_frozen_nets()\n",
    "    print(f'update frozen nets, {epoch} learn episode finished')\n",
    "\n",
    "    one_episode_return = np.sum(rewards)\n",
    "    one_epoch_termination = terminations\n",
    "    \n",
    "    #do some more prints for analyzing the model behavior while training\n",
    "    print(f'epoch {epoch + 1}, total_timesteps {total_timesteps}, actor loss {actor_loss}, critic loss {critic_loss}, avg_return {one_episode_return}')\n",
    "    \n",
    "    # Log metrics at the end of each epoch to tensorboard\n",
    "    log_metrics(epoch = epoch, total_timesteps = total_timesteps,\n",
    "                termination  = termination,\n",
    "                critic_loss = critic_loss, actor_loss= actor_loss,\n",
    "                episode_return = one_episode_return,\n",
    "                actor_learning_rate = agent.actor_learning_rate, critic_learning_rate = agent.critic_learning_rate,\n",
    "                epsilon = agent.epsilon, gamma = agent.gamma)\n",
    "    \n",
    "    # do prints every 10 epochs\n",
    "    # if (i + 1) % 10 == 0:\n",
    "    #     avg_return = compute_avg_return(env, agent, num_episodes=1)\n",
    "    #     print(f'epoch {i + 1}, total_timesteps {total_timesteps}, actor loss {actor_loss}, critic loss {critic_loss}, avg_return {avg_return}')\n",
    "    \n",
    "env.close() # kill gui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db997f26",
   "metadata": {},
   "source": [
    "# Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f418f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to h5 format\n",
    "# filepath_actor = f\"model/default_CartPole_{datum_uhrzeit}/actor.h5\"\n",
    "filepath_actor = f\"{savedir}\\\\actor.h5\"\n",
    "\n",
    "#filepath_critic = f\"model/default_CartPole_{datum_uhrzeit}/critic.h5\"\n",
    "filepath_critic = f\"{savedir}\\\\critic.h5\"\n",
    "\n",
    "agent.save_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27870f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 2] Unable to open file (unable to open file: name = 'model/default_CartPole_20240103_222230_actor.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
     ]
    }
   ],
   "source": [
    "# load the model from h5 format -> use new agent in new instance of the enviroment to prevent overwriting\n",
    "load_env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
    "\n",
    "load_agent = PPOAgent(env.action_space, env.observation_space)\n",
    "load_agent._init_networks()\n",
    "\n",
    "filepath_actor = f\"model/default_CartPole_{datum_uhrzeit}_actor.h5\"\n",
    "filepath_critic = f\"model/default_CartPole_{datum_uhrzeit}_critic.h5\"\n",
    "\n",
    "load_agent.load_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6dbba9",
   "metadata": {},
   "source": [
    "Rendering with pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a3107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reloaded sucessful\n",
      "Closed Rendering sucessful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Method to render an agent in a specific gym enviroment in external GUI -> not so laggy like matplotlib implementation\n",
    "def render_GUI(render_env, render_agent, filepath_actor, filepath_critic):\n",
    "\n",
    "    render_agent.load_models(filepath_actor, filepath_critic)\n",
    "    render_agent._init_networks()\n",
    "    render_obs,_ = render_env.reset()\n",
    "\n",
    "    # Start rendering in pyglet GUI (internal gym method which uses pyglet inthe background)\n",
    "    render_env.render()\n",
    "\n",
    "    #Run the GUI until Keyboard Interrupt hits (only cell-interrupt in Jupyter-Notebook is supported. It's not possible to close the GUI directly!)\n",
    "    try:\n",
    "        while True:\n",
    "            render_action = render_agent.act(np.array([render_obs]))\n",
    "            _, _, termination, truncation, _ = render_env.step(render_action)\n",
    "\n",
    "            if termination or truncation:\n",
    "                render_obs = render_env.reset()\n",
    "\n",
    "    except KeyboardInterrupt as e:\n",
    "        print('Closed Rendering sucessful')\n",
    "        render_env.close()\n",
    "\n",
    "# Set up the enviroment and load the trained agent from directory\n",
    "render_env = gym.make('MountainCarContinuous-v0', render_mode = 'human')\n",
    "render_agent = PPOAgent(render_env.action_space, render_env.observation_space)\n",
    "\n",
    "filepath_actor = f\"model/default_MountainCarContinuous_20240104_141521/actor.h5\"\n",
    "filepath_critic = f\"model/default_MountainCarContinuous_20240104_141521/critic.h5\"\n",
    "\n",
    "#call the function\n",
    "render_GUI(render_env, render_agent, filepath_actor, filepath_critic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5acb6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_avg_return' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m compute_avg_return(render_env, render_agent, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'compute_avg_return' is not defined"
     ]
    }
   ],
   "source": [
    "compute_avg_return(render_env, render_agent, num_episodes=1, render=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
