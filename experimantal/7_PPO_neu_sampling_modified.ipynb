{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b358710c-7fbd-482a-84a8-d220cda6de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras.callbacks import TensorBoard\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import pygame\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041b7af-9a74-45db-b58b-d4433987d60f",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "PPO is a policy gradient Actor-Critic algorithm. The policy model, the **actor** network  produces a stochastic policy. It maps the state to a probability distribution over the set of possible actions. The **critic** network is used to approximate the value function and then, the advantage is calculated:\n",
    "\n",
    "$$\n",
    "A_\\Phi (s_t, a_t) = q_\\Phi (s_t,a_t) - v_\\Phi (s_t) = R_t + \\gamma v_{\\Phi'} (s_{t+1}) - v_\\Phi (s_t)\n",
    "$$\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b5d89-2006-432e-a7fd-9c6eb930c5b0",
   "metadata": {},
   "source": [
    "PPO uses 2 main models. The actor network learns the stochastic policy. It maps the state to a probability distribution over the set of possible actions. The critic network learns the value function. It maps the state to a scalar.\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9179d247-1c6d-4307-bea3-def396311e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):    #change inheritance class to enable saving the models\n",
    "    def __init__(self, units=(400, 300), n_actions=2, **kwargs):\n",
    "        super(Actor, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(n_actions, activation='softmax'))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, units=(400, 300), **kwargs):\n",
    "        super(Critic, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(1))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class PPOAgent:\n",
    "    def __init__(self, action_space, observation_space, gamma=0.99, epsilon = 0.1):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.observation_shape = observation_space.shape[0]\n",
    "        self.actor = Actor(n_actions=action_space.n)\n",
    "        self.actor_old = Actor(n_actions=action_space.n)\n",
    "        self.critic = Critic()\n",
    "        self.target_critic = Critic()\n",
    "        \n",
    "        self.actor_learning_rate=0.00025\n",
    "        self.critic_learning_rate=0.001\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(self.actor_learning_rate)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.critic_learning_rate)   # default = 0,001 -> hatten wir auch schon\n",
    "        self._init_networks()\n",
    "        \n",
    "    def _init_networks(self):\n",
    "        initializer = np.zeros([1, self.observation_shape])   # ergänzt zu V1 -> hatten wir aber auch schon gemacht\n",
    "        self.actor(initializer)\n",
    "        self.actor_old(initializer)\n",
    "        \n",
    "        self.critic(initializer)\n",
    "        self.target_critic(initializer)\n",
    "        \n",
    "        self.update_frozen_nets()\n",
    "        \n",
    "    def act(self, observation):\n",
    "        probs = self.actor(observation).numpy()\n",
    "        probs = np.squeeze(probs)\n",
    "        action = np.random.choice(self.action_space.n, p=probs)\n",
    "        return action\n",
    "    \n",
    "    def get_critic_grads(self, states, rewards, next_states, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_value = self.target_critic(next_states)\n",
    "            q_value = rewards + (1-dones) * self.gamma * next_value\n",
    "            value = self.critic(states)\n",
    "            \n",
    "            advantage = q_value - value\n",
    "            loss = tf.reduce_mean(tf.square(advantage))\n",
    "        gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        return gradients, loss, advantage\n",
    "    \n",
    "    def get_actor_grads(self, states, actions, advantage):\n",
    "        with tf.GradientTape() as tape:\n",
    "            p_current = tf.gather(self.actor(states), actions, axis=1)\n",
    "            p_old = tf.gather(self.actor_old(states), actions, axis=1)\n",
    "            ratio = p_current / p_old\n",
    "            clip_ratio = tf.clip_by_value(ratio, 1-self.epsilon, 1+self.epsilon)\n",
    "            # entropy loss hatten wir probiert, bringt aber wenig --> sollte eigentlich exploration förndern\n",
    "            # standardize advantage\n",
    "            advantage = (advantage - tf.reduce_mean(advantage)) / (tf.keras.backend.std(advantage) + 1e-8)\n",
    "            objective = ratio * advantage\n",
    "            clip_objective = clip_ratio * advantage\n",
    "            loss = -tf.reduce_mean(tf.where(objective < clip_objective, objective, clip_objective))\n",
    "        gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        return gradients, loss\n",
    "        \n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        critic_grads, critic_loss, advantage = self.get_critic_grads(states, rewards, next_states, dones)\n",
    "        actor_grads, actor_loss = self.get_actor_grads(states, actions, advantage)\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def update_frozen_nets(self):\n",
    "        # TODO: set discount factor  -> was soll der hier bringen?\n",
    "        weights = self.actor.get_weights()\n",
    "        self.actor_old.set_weights(weights)\n",
    "        \n",
    "        weights = self.critic.get_weights()\n",
    "        self.target_critic.set_weights(weights)\n",
    "\n",
    "    def save_models(self, actor_path='actor_weights.h5', critic_path='critic_weights.h5'):\n",
    "        self.actor.save_weights(actor_path)\n",
    "        self.critic.save_weights(critic_path)\n",
    "\n",
    "    def load_models(self, actor_path='actor_weights.h5', critic_path='critic_weights.h5'):\n",
    "        try:\n",
    "            self.actor.load_weights(actor_path)\n",
    "            self.critic.load_weights(critic_path)\n",
    "            print('Model loaded sucessful')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f274993-caca-4c6d-b92a-ba5589d87831",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "PPO is an on-policy method. We allways complete a full episode, record the trajectory and the rewards. We then use these to update our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba5c53a-7ef2-4876-bf80-1bfc696e0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=1000, render=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            obs, r, termination, truncation, _ = env.step(action)\n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "        #print('episode_return', episode_return)\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a4a1a",
   "metadata": {},
   "source": [
    "## Add a Connection to Tensorboard -> online visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557bc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refers to log data and model data -> below for model data\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'model\\\\CustomMountainCarVelocity_mod_sampling_5-16-8_{datum_uhrzeit}'\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76892352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory model\\CustomMountainCarVelocity_mod_sampling_5-16-8_20240107_120820\\log exists.\n",
      "c:\\#_FESTPLATTE\\06_Studium\\Master_HKA\\Semesterdateien\\Semester 1\\Roboterprogrammierung-Hein\\Projekt\\Coding\\model\\CustomMountainCarVelocity_mod_sampling_5-16-8_20240107_120820\\log\n"
     ]
    }
   ],
   "source": [
    "log_dir = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    print(f\"The directory {log_dir} exists.\")\n",
    "    absolute_path = os.path.abspath(log_dir)\n",
    "    print(absolute_path)\n",
    "else:\n",
    "    print(f\"The directory {log_dir} does not exist.\")\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# define all metrics to log\n",
    "def log_metrics(epoch, total_timesteps, critic_loss, actor_loss, episode_return, actor_learning_rate, critic_learning_rate, epsilon, gamma):\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('epoch', epoch, step=epoch)\n",
    "        tf.summary.scalar('total_timesteps', total_timesteps, step=epoch)\n",
    "        tf.summary.scalar('critic_loss', critic_loss, step=epoch)\n",
    "        tf.summary.scalar('actor_loss', actor_loss, step=epoch)\n",
    "        tf.summary.scalar('episode_return', episode_return, step=epoch)\n",
    "        tf.summary.scalar('critic_learning_rate', critic_learning_rate, step=epoch)\n",
    "        tf.summary.scalar('actor_learning_rate', actor_learning_rate, step=epoch)\n",
    "        tf.summary.scalar('discount_factor_gamma', gamma, step=epoch)\n",
    "        tf.summary.scalar('clip_range_epsilon', epsilon, step=epoch)\n",
    "\n",
    "# tensorboard --logdir log in bash to open the online editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3cf9135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap custom enviroment with changed reward strategies\n",
    "class CustomMountainCarPosition(gym.Wrapper):       # Reward wird vergeben je höher car kommt\n",
    "    def __init__(self, inheritance_env):\n",
    "        super(CustomMountainCarPosition, self).__init__(inheritance_env)\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)    # \"normale\" step methode aufrufen\n",
    " \n",
    "        x_position = observation[0]\n",
    "        if x_position >= -0.5:\n",
    "            exp = int((x_position + 0.5) * 10)  # Exponent berechnen (z.B. -0.4 -> 0, -0.3 -> 1, ..., 0,4 -> 8)\n",
    "            reward = 2 ** exp\n",
    "            #print(\"Reward:\", reward)\n",
    "\n",
    "        # elif x_position >= -0.6:                \n",
    "        #     exp = int((-x_position - 0.6) * 10)  # Exponent berechnen (z.B. -0,6 -> 0, -0,7 -> 1, ..., -1,2 -> 6)\n",
    "        #     reward = 2 ** exp\n",
    "\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9c4e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMountainCarVelocity(gym.Wrapper):             # Reward wird vergeben wenn velocity hoch ist\n",
    "    def __init__(self, inheritance_env):\n",
    "        super(CustomMountainCarVelocity, self).__init__(inheritance_env)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)    # \"normale\" step methode aufrufen\n",
    "\n",
    "        #print(\"Velocity: \", observation[1])\n",
    "        if observation[1] < -0.02 or observation[1] > 0.02:\n",
    "            reward = 2\n",
    "\n",
    "        if observation[1] < -0.03 or observation[1] > 0.03:\n",
    "            reward = 4\n",
    "\n",
    "        if observation[1] < -0.04 or observation[1] > 0.04:\n",
    "            reward = 8\n",
    "\n",
    "        if observation[1] < -0.05 or observation[1] > 0.05:\n",
    "            reward = 16\n",
    "\n",
    "        if observation[1] < -0.06 or observation[1] > 0.06:\n",
    "            reward = 32\n",
    "\n",
    "        if terminated:\n",
    "            reward = 1000\n",
    "            #print(\"######## Terminated ########\")\n",
    "\n",
    "        return observation, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff7e92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env = gym.make('MountainCar-v0', render_mode='rgb_array')  #human fur pygame gui -> very laggy!\n",
    "\n",
    "inheritance_env = gym.make(\"MountainCar-v0\")\n",
    "env = CustomMountainCarVelocity(inheritance_env)\n",
    "\n",
    "agent = PPOAgent(env.action_space, env.observation_space)   # observation_space entpacken für obserbation_shape (siehe __init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5119ca2a-ef65-4676-b51b-41743eb2a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\mountain_car.py:173: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, total_timesteps 10000, actor loss 0.003568469313904643, critic loss 0.0006245007971301675, avg_return -200.0\n",
      "epoch 20, total_timesteps 20000, actor loss -0.04550672695040703, critic loss 0.025152314454317093, avg_return -200.0\n",
      "epoch 30, total_timesteps 30000, actor loss -0.012108182534575462, critic loss 0.008307019248604774, avg_return -200.0\n",
      "epoch 40, total_timesteps 40000, actor loss -0.028688006103038788, critic loss 0.21905425190925598, avg_return -200.0\n",
      "epoch 50, total_timesteps 50000, actor loss -0.02108853869140148, critic loss 0.08991749584674835, avg_return -200.0\n",
      "epoch 60, total_timesteps 60000, actor loss -0.025161752477288246, critic loss 0.030206799507141113, avg_return -200.0\n",
      "epoch 70, total_timesteps 70000, actor loss -0.015246591530740261, critic loss 0.029146483168005943, avg_return -200.0\n",
      "epoch 80, total_timesteps 80000, actor loss 0.014826688915491104, critic loss 52.51908493041992, avg_return -200.0\n",
      "epoch 90, total_timesteps 90000, actor loss -0.006715798284858465, critic loss 0.00935453362762928, avg_return -200.0\n",
      "epoch 100, total_timesteps 100000, actor loss 0.009545216336846352, critic loss 0.08332954347133636, avg_return -200.0\n",
      "epoch 110, total_timesteps 110000, actor loss -0.023926539346575737, critic loss 0.054860591888427734, avg_return -200.0\n",
      "epoch 120, total_timesteps 120000, actor loss 0.0026256083510816097, critic loss 0.17754605412483215, avg_return -200.0\n",
      "epoch 130, total_timesteps 130000, actor loss -0.007058980409055948, critic loss 0.052028823643922806, avg_return -200.0\n",
      "epoch 140, total_timesteps 140000, actor loss -0.02962816320359707, critic loss 0.07033751159906387, avg_return -200.0\n",
      "epoch 150, total_timesteps 150000, actor loss -0.0036545563489198685, critic loss 101.59397888183594, avg_return -200.0\n",
      "epoch 160, total_timesteps 160000, actor loss 0.004811923485249281, critic loss 1.906548261642456, avg_return -200.0\n",
      "epoch 170, total_timesteps 170000, actor loss -0.007613973692059517, critic loss 0.29689842462539673, avg_return -200.0\n",
      "epoch 180, total_timesteps 180000, actor loss -0.015463104471564293, critic loss 0.9644374847412109, avg_return -200.0\n",
      "epoch 190, total_timesteps 190000, actor loss -0.015623178333044052, critic loss 0.4233676493167877, avg_return -200.0\n",
      "epoch 200, total_timesteps 200000, actor loss -0.019279785454273224, critic loss 95.51754760742188, avg_return -200.0\n",
      "epoch 210, total_timesteps 210000, actor loss -0.0053009940311312675, critic loss 0.11874210834503174, avg_return -164.0\n",
      "epoch 220, total_timesteps 220000, actor loss -0.03309134393930435, critic loss 0.4660751223564148, avg_return -109.0\n",
      "epoch 230, total_timesteps 230000, actor loss 0.024154892191290855, critic loss 0.27784663438796997, avg_return -179.0\n",
      "epoch 240, total_timesteps 240000, actor loss -0.008438139222562313, critic loss 0.6381179094314575, avg_return -116.0\n",
      "epoch 250, total_timesteps 250000, actor loss -0.014350256882607937, critic loss 1.5086431503295898, avg_return -126.0\n",
      "epoch 260, total_timesteps 260000, actor loss -0.005393271334469318, critic loss 1.365084171295166, avg_return -41.0\n",
      "epoch 270, total_timesteps 270000, actor loss -0.010146631859242916, critic loss 15.686895370483398, avg_return 58.0\n",
      "epoch 280, total_timesteps 280000, actor loss -0.01002784725278616, critic loss 3.4141716957092285, avg_return -119.0\n",
      "epoch 290, total_timesteps 290000, actor loss -0.0053322408348321915, critic loss 39.2193489074707, avg_return -200.0\n"
     ]
    }
   ],
   "source": [
    "# memics settings from stable baselines -> goal faster computing\n",
    "n_rollouts = 5 \n",
    "batch_size = 64\n",
    "learn_steps = 8     \n",
    "total_timesteps = 0\n",
    "step_counter = 0\n",
    "\n",
    "#goal = 600.000 timesteps -> baseline has learned properly here\n",
    "for epoch in range(290):    # one epoch -> one complete learning iteration bzw. one update of the frozen nets\n",
    "    \n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    \n",
    "    # collect experience in the enviroment with current policy for n episodes/ rollouts\n",
    "    # give the agent more time to collect experiences more apart from the starting state\n",
    "    for rollouts in range(n_rollouts):  \n",
    "        obs, _ = env.reset()\n",
    "        done = False  \n",
    "\n",
    "        while not done:\n",
    "            env.render()    # call gui if render_mode = 'human'\n",
    "            action = agent.act(np.array([obs]))\n",
    "            new_obs, r, termination, truncation, _ = env.step(action)\n",
    "            if termination:\n",
    "                print('######TERMINATED######')\n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "\n",
    "            states.append(obs)\n",
    "            rewards.append([r])\n",
    "            actions.append([action])\n",
    "            obs = new_obs\n",
    "            next_states.append(obs)\n",
    "            dones.append([done])\n",
    "\n",
    "            total_timesteps += 1\n",
    "            step_counter += 1\n",
    "    \n",
    "    step_counter = 0\n",
    "    # store colledted experience for all rollouts/ episodes and reset enviroment for next episode/ rollout\n",
    "    states, actions, rewards, next_states, dones = map(np.array, [states, actions, rewards, next_states, dones])\n",
    "    obs, _ = env.reset()\n",
    "    #print('collected experience in n rollouts')\n",
    "\n",
    "    # learn policy and value from the collected data \n",
    "    for _ in range(learn_steps):\n",
    "        indices = np.arange(states.shape[0])\n",
    "        np.random.shuffle(indices)  # create random indice row\n",
    "        \n",
    "        # switch indices to random experience distribution\n",
    "        shuffled_states = states[indices]\n",
    "        shuffled_actions = actions[indices]\n",
    "        shuffled_rewards = rewards[indices]\n",
    "        shuffled_next_states = next_states[indices]\n",
    "        shuffled_dones = dones[indices]\n",
    "\n",
    "        # divides the whole shuffled experience into batches of batch_size\n",
    "        for j in range(0, states.shape[0], batch_size):\n",
    "            states_batch = shuffled_states[j:j + batch_size]    # j:j + batch_size -> returns all elements from x*batch_size to (x+1)*batch_size\n",
    "            actions_batch = shuffled_actions[j:j + batch_size]\n",
    "            rewards_batch = shuffled_rewards[j:j + batch_size]\n",
    "            next_states_batch = shuffled_next_states[j:j + batch_size]\n",
    "            dones_batch = shuffled_dones[j:j + batch_size]\n",
    "\n",
    "            # push one batch of the shuffled experience to the learning method -> one update of the current nets (actor and critic) per passed batch of experience\n",
    "            actor_loss, critic_loss = agent.learn(states_batch,\n",
    "                                                  actions_batch,\n",
    "                                                  rewards_batch,\n",
    "                                                  next_states_batch,\n",
    "                                                  dones_batch)\n",
    "            #print('update online nets, learn from one batch')\n",
    "    agent.update_frozen_nets()\n",
    "    #print('update frozen nets, do one learn step')\n",
    "    one_episode_return = compute_avg_return(env, agent, num_episodes=1)\n",
    "    print(f'epoch {epoch + 1}, total_timesteps {total_timesteps}, actor loss {actor_loss}, critic loss {critic_loss}, _return {one_episode_return}')\n",
    "    # Log metrics at the end of each epoch\n",
    "    log_metrics(epoch = epoch, total_timesteps = total_timesteps,\n",
    "                critic_loss = critic_loss, actor_loss= actor_loss,\n",
    "                episode_return = one_episode_return,\n",
    "                actor_learning_rate = agent.actor_learning_rate, critic_learning_rate = agent.critic_learning_rate,\n",
    "                epsilon = agent.epsilon, gamma = agent.gamma)\n",
    "    \n",
    "    # do prints every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_return = compute_avg_return(env, agent, num_episodes=1)\n",
    "        print(f'epoch {epoch + 1}, total_timesteps {total_timesteps}, actor loss {actor_loss}, critic loss {critic_loss}, avg_return {avg_return}')\n",
    "    \n",
    "env.close() # kill gui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db997f26",
   "metadata": {},
   "source": [
    "# Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f418f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to h5 format\n",
    "#filepath_actor = f\"model/default_CartPole_{datum_uhrzeit}/actor.h5\"\n",
    "filepath_actor = f\"{savedir}\\\\actor.h5\"\n",
    "\n",
    "#filepath_critic = f\"model/default_CartPole_{datum_uhrzeit}/critic.h5\"\n",
    "filepath_critic = f\"{savedir}\\\\critic.h5\"\n",
    "\n",
    "agent.save_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27870f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded sucessful\n"
     ]
    }
   ],
   "source": [
    "# load the model from h5 format -> use new agent in new instance of the enviroment to prevent overwriting\n",
    "load_env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
    "\n",
    "load_agent = PPOAgent(load_env.action_space, load_env.observation_space)\n",
    "load_agent._init_networks()\n",
    "\n",
    "filepath_actor = f\"model\\\\CustomMountainCarVelocity_mod_sampling_5-16-8_20240107_120820\\\\actor.h5\"\n",
    "filepath_critic = f\"model\\\\CustomMountainCarVelocity_mod_sampling_5-16-8_20240107_120820\\\\critic.h5\"\n",
    "\n",
    "load_agent.load_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2cf59",
   "metadata": {},
   "source": [
    "render enviroment without pygame --> instead with matplotlib , a little bit laggy but ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf9fff65-511c-46e2-b01c-0290e7f0548e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m compute_avg_return(load_env, load_agent, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m load_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m, in \u001b[0;36mcompute_avg_return\u001b[1;34m(env, agent, num_episodes, max_steps, render)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[0;32m     10\u001b[0m     clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 11\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(env\u001b[38;5;241m.\u001b[39mrender())\n\u001b[0;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:2413\u001b[0m, in \u001b[0;36maxis\u001b[1;34m(arg, emit, **kwargs)\u001b[0m\n\u001b[0;32m   2411\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   2412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maxis\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39m, emit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39maxis(arg, emit\u001b[38;5;241m=\u001b[39memit, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:2309\u001b[0m, in \u001b[0;36mgca\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39mgca)\n\u001b[0;32m   2308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgca\u001b[39m():\n\u001b[1;32m-> 2309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gcf()\u001b[38;5;241m.\u001b[39mgca()\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\figure.py:1660\u001b[0m, in \u001b[0;36mFigureBase.gca\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1650\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1651\u001b[0m \u001b[38;5;124;03mGet the current Axes.\u001b[39;00m\n\u001b[0;32m   1652\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;124;03mwhether `.pyplot.get_fignums()` is empty.)\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_axstack\u001b[38;5;241m.\u001b[39mcurrent()\n\u001b[1;32m-> 1660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ax \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_subplot()\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\figure.py:768\u001b[0m, in \u001b[0;36mFigureBase.add_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    765\u001b[0m         args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])))\n\u001b[0;32m    766\u001b[0m     projection_class, pkw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_projection_requirements(\n\u001b[0;32m    767\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 768\u001b[0m     ax \u001b[38;5;241m=\u001b[39m projection_class(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpkw)\n\u001b[0;32m    769\u001b[0m     key \u001b[38;5;241m=\u001b[39m (projection_class, pkw)\n\u001b[0;32m    770\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_axes_internal(ax, key)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:683\u001b[0m, in \u001b[0;36m_AxesBase.__init__\u001b[1;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, *args, **kwargs)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_axisbelow(mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.axisbelow\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rasterization_zorder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    685\u001b[0m \u001b[38;5;66;03m# funcs used to format x and y - fall back on major formatters\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt_xdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:1395\u001b[0m, in \u001b[0;36m_AxesBase.clear\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1393\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcla()\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__clear()\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:1312\u001b[0m, in \u001b[0;36m_AxesBase.__clear\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegend_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Disable grid on init to use rcParameter\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gridOn, which\u001b[38;5;241m=\u001b[39mmpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.grid.which\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   1314\u001b[0m           axis\u001b[38;5;241m=\u001b[39mmpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.grid.axis\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m   1315\u001b[0m props \u001b[38;5;241m=\u001b[39m font_manager\u001b[38;5;241m.\u001b[39mFontProperties(\n\u001b[0;32m   1316\u001b[0m     size\u001b[38;5;241m=\u001b[39mmpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.titlesize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   1317\u001b[0m     weight\u001b[38;5;241m=\u001b[39mmpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes.titleweight\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:3196\u001b[0m, in \u001b[0;36m_AxesBase.grid\u001b[1;34m(self, visible, which, axis, **kwargs)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mgrid(visible, which\u001b[38;5;241m=\u001b[39mwhich, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m-> 3196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mgrid(visible, which\u001b[38;5;241m=\u001b[39mwhich, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axis.py:1660\u001b[0m, in \u001b[0;36mAxis.grid\u001b[1;34m(self, visible, which, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m   1658\u001b[0m     gridkw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgridOn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major_tick_kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgridOn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1659\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m visible \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m visible)\n\u001b[1;32m-> 1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_tick_params(which\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgridkw)\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axis.py:947\u001b[0m, in \u001b[0;36mAxis.set_tick_params\u001b[1;34m(self, which, reset, **kwargs)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major_tick_kw\u001b[38;5;241m.\u001b[39mupdate(kwtrans)\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajorTicks:\n\u001b[0;32m    948\u001b[0m         tick\u001b[38;5;241m.\u001b[39m_apply_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwtrans)\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axis.py:595\u001b[0m, in \u001b[0;36m_LazyTickList.__get__\u001b[1;34m(self, instance, cls)\u001b[0m\n\u001b[0;32m    593\u001b[0m     instance\u001b[38;5;241m.\u001b[39mmajorTicks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    594\u001b[0m     tick \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39m_get_tick(major\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 595\u001b[0m     instance\u001b[38;5;241m.\u001b[39mmajorTicks\u001b[38;5;241m.\u001b[39mappend(tick)\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mmajorTicks\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axis.py:594\u001b[0m, in \u001b[0;36m_LazyTickList.__get__\u001b[1;34m(self, instance, cls)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major:\n\u001b[0;32m    593\u001b[0m     instance\u001b[38;5;241m.\u001b[39mmajorTicks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 594\u001b[0m     tick \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39m_get_tick(major\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    595\u001b[0m     instance\u001b[38;5;241m.\u001b[39mmajorTicks\u001b[38;5;241m.\u001b[39mappend(tick)\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mmajorTicks\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axis.py:1551\u001b[0m, in \u001b[0;36mAxis._get_tick\u001b[1;34m(self, major)\u001b[0m\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1548\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Axis subclass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must define \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tick_class or reimplement _get_tick()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1550\u001b[0m tick_kw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_major_tick_kw \u001b[38;5;28;01mif\u001b[39;00m major \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_minor_tick_kw\n\u001b[1;32m-> 1551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tick_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes, \u001b[38;5;241m0\u001b[39m, major\u001b[38;5;241m=\u001b[39mmajor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtick_kw)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axis.py:478\u001b[0m, in \u001b[0;36mYTick.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 478\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;66;03m# x in axes coords, y in data coords\u001b[39;00m\n\u001b[0;32m    480\u001b[0m     ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axis.py:183\u001b[0m, in \u001b[0;36mTick.__init__\u001b[1;34m(self, axes, loc, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major, labelrotation, grid_color, grid_linestyle, grid_linewidth, grid_alpha, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel1 \u001b[38;5;241m=\u001b[39m mtext\u001b[38;5;241m.\u001b[39mText(\n\u001b[0;32m    175\u001b[0m     np\u001b[38;5;241m.\u001b[39mnan, np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m    176\u001b[0m     fontsize\u001b[38;5;241m=\u001b[39mlabelsize, color\u001b[38;5;241m=\u001b[39mlabelcolor, visible\u001b[38;5;241m=\u001b[39mlabel1On,\n\u001b[0;32m    177\u001b[0m     rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labelrotation[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2 \u001b[38;5;241m=\u001b[39m mtext\u001b[38;5;241m.\u001b[39mText(\n\u001b[0;32m    179\u001b[0m     np\u001b[38;5;241m.\u001b[39mnan, np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m    180\u001b[0m     fontsize\u001b[38;5;241m=\u001b[39mlabelsize, color\u001b[38;5;241m=\u001b[39mlabelcolor, visible\u001b[38;5;241m=\u001b[39mlabel2On,\n\u001b[0;32m    181\u001b[0m     rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labelrotation[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_tickdir(tickdir)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m artist \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick1line, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick2line, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgridline,\n\u001b[0;32m    186\u001b[0m                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel2]:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_artist_props(artist)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\axis.py:513\u001b[0m, in \u001b[0;36mYTick._apply_tickdir\u001b[1;34m(self, tickdir)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_apply_tickdir(tickdir)\n\u001b[0;32m    508\u001b[0m mark1, mark2 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m: (mlines\u001b[38;5;241m.\u001b[39mTICKLEFT, mlines\u001b[38;5;241m.\u001b[39mTICKRIGHT),\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m: (mlines\u001b[38;5;241m.\u001b[39mTICKRIGHT, mlines\u001b[38;5;241m.\u001b[39mTICKLEFT),\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minout\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    512\u001b[0m }[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tickdir]\n\u001b[1;32m--> 513\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick1line\u001b[38;5;241m.\u001b[39mset_marker(mark1)\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtick2line\u001b[38;5;241m.\u001b[39mset_marker(mark2)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\lines.py:1192\u001b[0m, in \u001b[0;36mLine2D.set_marker\u001b[1;34m(self, marker)\u001b[0m\n\u001b[0;32m   1181\u001b[0m \u001b[38;5;129m@_docstring\u001b[39m\u001b[38;5;241m.\u001b[39minterpd\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_marker\u001b[39m(\u001b[38;5;28mself\u001b[39m, marker):\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;124;03m    Set the line marker.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;124;03m        arguments.\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_marker \u001b[38;5;241m=\u001b[39m MarkerStyle(marker, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_marker\u001b[38;5;241m.\u001b[39mget_fillstyle())\n\u001b[0;32m   1193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\markers.py:272\u001b[0m, in \u001b[0;36mMarkerStyle.__init__\u001b[1;34m(self, marker, fillstyle, transform, capstyle, joinstyle)\u001b[0m\n\u001b[0;32m    267\u001b[0m     marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.6\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarkerStyle(None) is deprecated since \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; support will be removed \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.  Use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarkerStyle(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) to construct an empty MarkerStyle.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_marker(marker)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\markers.py:367\u001b[0m, in \u001b[0;36mMarkerStyle._set_marker\u001b[1;34m(self, marker)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(marker, MarkerStyle):\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_marker \u001b[38;5;241m=\u001b[39m marker\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recache()\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\markers.py:291\u001b[0m, in \u001b[0;36mMarkerStyle._recache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Initial guess: Assume the marker is filled unless the fillstyle is\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# set to 'none'. The marker function will override this for unfilled\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# markers.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fillstyle \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_marker_function()\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\markers.py:792\u001b[0m, in \u001b[0;36mMarkerStyle._set_tickleft\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_tickleft\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform \u001b[38;5;241m=\u001b[39m Affine2D()\u001b[38;5;241m.\u001b[39mscale(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_snap_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\Lib\\site-packages\\matplotlib\\transforms.py:1903\u001b[0m, in \u001b[0;36mAffine2D.__init__\u001b[1;34m(self, matrix, **kwargs)\u001b[0m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1901\u001b[0m     \u001b[38;5;66;03m# A bit faster than np.identity(3).\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m IdentityTransform\u001b[38;5;241m.\u001b[39m_mtx\n\u001b[1;32m-> 1903\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mtx \u001b[38;5;241m=\u001b[39m matrix\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_avg_return(load_env, load_agent, num_episodes=20, render=True)\n",
    "load_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
