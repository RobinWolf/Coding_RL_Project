{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b358710c-7fbd-482a-84a8-d220cda6de35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.15.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras.callbacks import TensorBoard\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import pygame\n",
    "\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041b7af-9a74-45db-b58b-d4433987d60f",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "PPO is a policy gradient Actor-Critic algorithm. The policy model, the **actor** network  produces a stochastic policy. It maps the state to a probability distribution over the set of possible actions. The **critic** network is used to approximate the value function and then, the advantage is calculated:\n",
    "\n",
    "$$\n",
    "A_\\Phi (s_t, a_t) = q_\\Phi (s_t,a_t) - v_\\Phi (s_t) = R_t + \\gamma v_{\\Phi'} (s_{t+1}) - v_\\Phi (s_t)\n",
    "$$\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b5d89-2006-432e-a7fd-9c6eb930c5b0",
   "metadata": {},
   "source": [
    "PPO uses 2 main models. The actor network learns the stochastic policy. It maps the state to a probability distribution over the set of possible actions. The critic network learns the value function. It maps the state to a scalar.\n",
    "\n",
    "The critic, $v_\\Phi$ is trained in the same manner, as the DQN model and the critic of DDPG, with TD-learning and a \"frozen\" and periodically updated target critic network, $v_{\\Phi'}$. Instead of approximating a q-value, it approximates the value.\n",
    "\n",
    "To train the actor, PPO uses the ratio of two policies:\n",
    "- a current policy $\\pi_\\Theta$, that is learned currently\n",
    "- a baseline policy $\\pi_{\\Theta´}$, an earlier version of the policy\n",
    "\n",
    "$$\n",
    "r^t (\\Theta)=r_\\Theta (s_t,a_t) = \\frac{\\pi_\\Theta (a_t | s_t)}{\\pi_{\\Theta'} (a_t | s_t)}\n",
    "$$\n",
    "\n",
    "It is the ratio of the probabilities of selecting $a_t$ given $\\pi_\\Theta$ and the probability of selecting the same action with $\\pi_{\\Theta´}$.\n",
    "\n",
    "When multiplied with the the approximated advantage, calculated using the critic network, it can be used as the objective function (maximize with SGA)\n",
    "\n",
    "$$\n",
    "loss_{actor} = - r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t)\n",
    "$$\n",
    "\n",
    "as when\n",
    "- the advantage is positive, meaning, that selecting the action would increase the value, the probability of selecting this action would increase\n",
    "- the advantage is negative, meaning, that selecting the action would decrease the value, the probability of selecting this action would decrease\n",
    "\n",
    "Instead of using this directly as loss function, to stabilize the implementation by adjusting the policy optimization step size, the loss is extended in a pessimistic way:\n",
    "\n",
    "$$\n",
    "loss_{actor} = \\min [r_\\Theta (s_t, a_t) A_\\Phi (s_t, a_t), clip(r_\\Theta (s_t, a_t), 1-\\epsilon, 1+\\epsilon) A_\\Phi (s_t, a_t)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9179d247-1c6d-4307-bea3-def396311e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):    #change inheritance class to enable saving the models\n",
    "    def __init__(self, units=(400, 300), n_actions=2, **kwargs):\n",
    "        super(Actor, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(n_actions, activation='softmax'))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, units=(400, 300), **kwargs):\n",
    "        super(Critic, self).__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        for i, u in enumerate(units):\n",
    "            self._layers.append(tf.keras.layers.Dense(u, activation='relu'))\n",
    "        self._layers.append(tf.keras.layers.Dense(1))\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = inputs\n",
    "        for l in self._layers:\n",
    "            outputs = l(outputs)\n",
    "        return outputs\n",
    "    \n",
    "class PPOAgent:\n",
    "    def __init__(self, action_space, observation_space, gamma=0.99, epsilon = 0.1):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.observation_shape = observation_space.shape[0]\n",
    "        self.actor = Actor(n_actions=action_space.n)\n",
    "        self.actor_old = Actor(n_actions=action_space.n)\n",
    "        self.critic = Critic()\n",
    "        self.target_critic = Critic()\n",
    "        \n",
    "        self.actor_learning_rate=0.00025\n",
    "        self.critic_learning_rate=0.001\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(self.actor_learning_rate)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.critic_learning_rate)   # default = 0,001 -> hatten wir auch schon\n",
    "        self._init_networks()\n",
    "        \n",
    "    def _init_networks(self):\n",
    "        initializer = np.zeros([1, self.observation_shape])   # ergänzt zu V1 -> hatten wir aber auch schon gemacht\n",
    "        self.actor(initializer)\n",
    "        self.actor_old(initializer)\n",
    "        \n",
    "        self.critic(initializer)\n",
    "        self.target_critic(initializer)\n",
    "        \n",
    "        self.update_frozen_nets()\n",
    "        \n",
    "    def act(self, observation):\n",
    "        probs = self.actor(observation).numpy()\n",
    "        probs = np.squeeze(probs)\n",
    "        action = np.random.choice(self.action_space.n, p=probs)\n",
    "        return action\n",
    "    \n",
    "    def get_critic_grads(self, states, rewards, next_states, dones):\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_value = self.target_critic(next_states)\n",
    "            q_value = rewards + (1-dones) * self.gamma * next_value\n",
    "            value = self.critic(states)\n",
    "            \n",
    "            advantage = q_value - value\n",
    "            loss = tf.reduce_mean(tf.square(advantage))\n",
    "        gradients = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        return gradients, loss, advantage\n",
    "    \n",
    "    def get_actor_grads(self, states, actions, advantage):\n",
    "        with tf.GradientTape() as tape:\n",
    "            p_current = tf.gather(self.actor(states), actions, axis=1)\n",
    "            p_old = tf.gather(self.actor_old(states), actions, axis=1)\n",
    "            ratio = p_current / p_old\n",
    "            clip_ratio = tf.clip_by_value(ratio, 1-self.epsilon, 1+self.epsilon)\n",
    "            # entropy loss hatten wir probiert, bringt aber wenig --> sollte eigentlich exploration förndern\n",
    "            # standardize advantage\n",
    "            advantage = (advantage - tf.reduce_mean(advantage)) / (tf.keras.backend.std(advantage) + 1e-8)\n",
    "            objective = ratio * advantage\n",
    "            clip_objective = clip_ratio * advantage\n",
    "            loss = -tf.reduce_mean(tf.where(objective < clip_objective, objective, clip_objective))\n",
    "        gradients = tape.gradient(loss, self.actor.trainable_variables)\n",
    "        return gradients, loss\n",
    "        \n",
    "\n",
    "    def learn(self, states, actions, rewards, next_states, dones):\n",
    "        critic_grads, critic_loss, advantage = self.get_critic_grads(states, rewards, next_states, dones)\n",
    "        actor_grads, actor_loss = self.get_actor_grads(states, actions, advantage)\n",
    "        \n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
    "        return actor_loss, critic_loss\n",
    "    \n",
    "    def update_frozen_nets(self):\n",
    "        # TODO: set discount factor  -> was soll der hier bringen?\n",
    "        weights = self.actor.get_weights()\n",
    "        self.actor_old.set_weights(weights)\n",
    "        \n",
    "        weights = self.critic.get_weights()\n",
    "        self.target_critic.set_weights(weights)\n",
    "\n",
    "    def save_models(self, actor_path='actor_weights.h5', critic_path='critic_weights.h5'):\n",
    "        self.actor.save_weights(actor_path)\n",
    "        self.critic.save_weights(critic_path)\n",
    "\n",
    "    def load_models(self, actor_path='actor_weights.h5', critic_path='critic_weights.h5'):\n",
    "        try:\n",
    "            self.actor.load_weights(actor_path)\n",
    "            self.critic.load_weights(critic_path)\n",
    "            print('Model loaded sucessful')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f274993-caca-4c6d-b92a-ba5589d87831",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "PPO is an on-policy method. We allways complete a full episode, record the trajectory and the rewards. We then use these to update our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba5c53a-7ef2-4876-bf80-1bfc696e0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(env, agent, num_episodes=1, max_steps=1000, render=False):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_return = 0.0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not (done or steps > max_steps):\n",
    "            if render:\n",
    "                clear_output(wait=True)\n",
    "                plt.axis('off')\n",
    "                plt.imshow(env.render())\n",
    "                plt.show()\n",
    "            action = agent.act(np.array([obs]))\n",
    "            obs, r, termination, truncation, _ = env.step(action)\n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "        total_return += episode_return\n",
    "        #print('episode_return', episode_return)\n",
    "    return total_return / num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a4a1a",
   "metadata": {},
   "source": [
    "## Add a Connection to Tensorboard -> online visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557bc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refers to log data and model data -> below for model data\n",
    "jetzt = datetime.datetime.now()\n",
    "datum_uhrzeit = jetzt.strftime(\"%Y%m%d_%H%M%S\")\n",
    "savedir = f'model\\\\default_MountainCar_{datum_uhrzeit}'\n",
    "os.makedirs('model', exist_ok=True)\n",
    "os.makedirs(savedir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76892352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory model\\default_MountainCar_20240103_222230\\log exists.\n",
      "c:\\#_FESTPLATTE\\06_Studium\\Master_HKA\\Semesterdateien\\Semester 1\\Roboterprogrammierung-Hein\\Projekt\\Coding\\model\\default_MountainCar_20240103_222230\\log\n"
     ]
    }
   ],
   "source": [
    "log_dir = f\"{savedir}\\\\log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "if os.path.exists(log_dir):\n",
    "    print(f\"The directory {log_dir} exists.\")\n",
    "    absolute_path = os.path.abspath(log_dir)\n",
    "    print(absolute_path)\n",
    "else:\n",
    "    print(f\"The directory {log_dir} does not exist.\")\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# define all metrics to log\n",
    "def log_metrics(epoch, total_timesteps, critic_loss, actor_loss, episode_return, actor_learning_rate, critic_learning_rate):\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('epoch', epoch, step=epoch)\n",
    "        tf.summary.scalar('total_timesteps', total_timesteps, step=epoch)\n",
    "        tf.summary.scalar('critic_loss', critic_loss, step=epoch)\n",
    "        tf.summary.scalar('actor_loss', actor_loss, step=epoch)\n",
    "        tf.summary.scalar('episode_return', episode_return, step=epoch)\n",
    "        tf.summary.scalar('critic_learning_rate', critic_learning_rate, step=epoch)\n",
    "        tf.summary.scalar('actor_learning_rate', actor_learning_rate, step=epoch)\n",
    "\n",
    "# tensorboard --logdir filepath in bash to open the online editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff7e92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\robin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode='rgb_array')  #human fur pygame gui -> very laggy!\n",
    "\n",
    "agent = PPOAgent(env.action_space, env.observation_space)   # observation_space entpacken für obserbation_shape (siehe __init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5119ca2a-ef65-4676-b51b-41743eb2a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rollouts = 5\n",
    "batch_size = 8\n",
    "learn_steps = 16\n",
    "total_timesteps = 0\n",
    "\n",
    "for i in range(150):    # one epoch -> one complete learning iteration bzw. one update of the frozen nets\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    \n",
    "    # collect experience in the enviroment with current policy for n episodes/ rollouts\n",
    "    # give the agent more time to collect experiences more apart from the starting state\n",
    "    for _ in range(n_rollouts):         \n",
    "        while not done:\n",
    "            env.render()    # call gui if render_mode = 'human'\n",
    "            action = agent.act(np.array([obs]))\n",
    "            new_obs, r, termination, truncation, _ = env.step(action)\n",
    "            if termination or truncation:\n",
    "                done = True\n",
    "\n",
    "            states.append(obs)\n",
    "            rewards.append([r])\n",
    "            actions.append([action])\n",
    "            obs = new_obs\n",
    "            next_states.append(obs)\n",
    "            dones.append([done])\n",
    "\n",
    "            total_timesteps += 1\n",
    "    \n",
    "    # store colledted experience for all rollouts/ episodes and reset enviroment for next episode/ rollout\n",
    "    states, actions, rewards, next_states, dones = map(np.array, [states, actions, rewards, next_states, dones])\n",
    "    obs, _ = env.reset()\n",
    "    #print('collected experience in n rollouts')\n",
    "\n",
    "    # learn policy and value from the collected data \n",
    "    for _ in range(learn_steps):\n",
    "        indices = np.arange(states.shape[0])\n",
    "        np.random.shuffle(indices)  # create random indice row\n",
    "        \n",
    "        # switch indices to random experience distribution\n",
    "        shuffled_states = states[indices]\n",
    "        shuffled_actions = actions[indices]\n",
    "        shuffled_rewards = rewards[indices]\n",
    "        shuffled_next_states = next_states[indices]\n",
    "        shuffled_dones = dones[indices]\n",
    "\n",
    "        # divides the whole shuffled experience into batches of batch_size\n",
    "        for j in range(0, states.shape[0], batch_size):\n",
    "            states_batch = shuffled_states[j:j + batch_size]    # j:j + batch_size -> returns all elements from x*batch_size to (x+1)*batch_size\n",
    "            actions_batch = shuffled_actions[j:j + batch_size]\n",
    "            rewards_batch = shuffled_rewards[j:j + batch_size]\n",
    "            next_states_batch = shuffled_next_states[j:j + batch_size]\n",
    "            dones_batch = shuffled_dones[j:j + batch_size]\n",
    "\n",
    "            # push one batch of the shuffled experience to the learning method -> one update of the current nets (actor and critic) per passed batch of experience\n",
    "            actor_loss, critic_loss = agent.learn(states_batch,\n",
    "                                                  actions_batch,\n",
    "                                                  rewards_batch,\n",
    "                                                  next_states_batch,\n",
    "                                                  dones_batch)\n",
    "            #print('update online nets, learn from one batch')\n",
    "    agent.update_frozen_nets()\n",
    "    #print('update frozen nets, do one learn step')\n",
    "    one_episode_return = compute_avg_return(env, agent, num_episodes=1)\n",
    "\n",
    "    # Log metrics at the end of each epoch\n",
    "    log_metrics(epoch = i, total_timesteps = total_timesteps,\n",
    "                critic_loss = critic_loss, actor_loss= actor_loss,\n",
    "                episode_return = one_episode_return,\n",
    "                actor_learning_rate = agent.actor_learning_rate, critic_learning_rate = agent.critic_learning_rate)\n",
    "    \n",
    "    # do prints every 10 epochs\n",
    "    if (i + 1) % 10 == 0:\n",
    "        avg_return = compute_avg_return(env, agent, num_episodes=1)\n",
    "        print(f'epoch {i + 1}, total_timesteps {total_timesteps}, actor loss {actor_loss}, critic loss {critic_loss}, avg_return {avg_return}')\n",
    "    \n",
    "env.close() # kill gui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db997f26",
   "metadata": {},
   "source": [
    "# Storing and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f418f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to h5 format\n",
    "#filepath_actor = f\"model/default_CartPole_{datum_uhrzeit}/actor.h5\"\n",
    "filepath_actor = f\"{savedir}\\\\actor.h5\"\n",
    "\n",
    "#filepath_critic = f\"model/default_CartPole_{datum_uhrzeit}/critic.h5\"\n",
    "filepath_critic = f\"{savedir}\\\\critic.h5\"\n",
    "\n",
    "agent.save_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27870f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from h5 format -> use new agent in new instance of the enviroment to prevent overwriting\n",
    "load_env = gym.make(\"MountainCar-v0\", render_mode='rgb_array')\n",
    "\n",
    "load_agent = PPOAgent(env.action_space, env.observation_space)\n",
    "load_agent._init_networks()\n",
    "\n",
    "filepath_actor = f\"model/default_CartPole_{datum_uhrzeit}_actor.h5\"\n",
    "filepath_critic = f\"model/default_CartPole_{datum_uhrzeit}_critic.h5\"\n",
    "\n",
    "load_agent.load_models(filepath_actor, filepath_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def store_models(agent, path='./model'):\n",
    "#     os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "#     actor_path = f'{path}/actor'\n",
    "#     agent.actor.save_weights(actor_path)\n",
    "    \n",
    "#     critic_path = f'{path}/critic'\n",
    "#     agent.critic.save_weights(critic_path)\n",
    "\n",
    "\n",
    "# def load_models(agent, path='./model'):\n",
    "#     actor_path = f'{path}/actor'\n",
    "#     if not os.path.exists(actor_path + '.index'):\n",
    "#         raise FileNotFoundError(f\"Actor model not found at {actor_path}.\")\n",
    "    \n",
    "#     critic_path = f'{path}/critic'\n",
    "#     if not os.path.exists(critic_path + '.index'):\n",
    "#         raise FileNotFoundError(f\"Critic model not found at {critic_path}.\")\n",
    "    \n",
    "#     agent.actor.load_weights(actor_path)\n",
    "#     agent.critic.load_weights(critic_path)\n",
    "#     agent.update_frozen_nets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2821f37",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1096183916.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[87], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    store_models(agent\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# timestamp = time.strftime(\"%Y_%m_%d--%H_%M\")\n",
    "# store_models(agent, path = f'./Trained_Models/{timestamp}_default_CartPole-V1')\n",
    "# store_models(agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to load the trained agent, initialize an agent\n",
    "# agent = PPOAgent(env.action_space, env.observation_space)\n",
    "# # and load the stored weights\n",
    "# load_models(agent, path='./model_default_CartPole-V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9fff65-511c-46e2-b01c-0290e7f0548e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_return 500.0\n",
      "episode_return 284.0\n",
      "episode_return 233.0\n",
      "episode_return 363.0\n",
      "episode_return 351.0\n",
      "episode_return 249.0\n",
      "episode_return 344.0\n",
      "episode_return 299.0\n",
      "episode_return 283.0\n",
      "episode_return 236.0\n",
      "episode_return 349.0\n",
      "episode_return 500.0\n",
      "episode_return 332.0\n",
      "episode_return 456.0\n",
      "episode_return 290.0\n",
      "episode_return 298.0\n",
      "episode_return 335.0\n",
      "episode_return 361.0\n",
      "episode_return 500.0\n",
      "episode_return 385.0\n"
     ]
    }
   ],
   "source": [
    "compute_avg_return(env, agent, num_episodes=20, render=False)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
